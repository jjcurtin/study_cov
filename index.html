<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kendra Wyant">
<meta name="author" content="Lauren Khoury">
<meta name="author" content="Markus Brauer">
<meta name="author" content="John J. Curtin">
<meta name="dcterms.date" content="2026-01-09">

<title>Evaluating Methods for Covariate Selection in Experimental Designs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-81e35ebdb4125010edbabe6010586085.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Evaluating Methods for Covariate Selection in Experimental Designs">
<meta name="citation_abstract" content="Abstract of paper goes here and can span several lines.
">
<meta name="citation_author" content="Kendra Wyant">
<meta name="citation_author" content="Lauren Khoury">
<meta name="citation_author" content="Markus Brauer">
<meta name="citation_author" content="John J. Curtin">
<meta name="citation_publication_date" content="2026-01-09">
<meta name="citation_cover_date" content="2026-01-09">
<meta name="citation_year" content="2026">
<meta name="citation_online_date" content="2026-01-09">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Choosing covariates in the analysis of clinical trials;,citation_abstract=Much of the literature on clinical trials emphasizes the importance of adjusting the results for any covariates (baseline variables) for which randomization fails to produce nearly exact balance, but the literature is very nearly devoid of recipes for assessing the consequences of such adjustments. Several years ago, Paul Canner presented an approximate expression for the effect of a covariate adjustment, and he considered its use in the selection of covariates. With the aid of Canner’s equation, using both formal analysis and simulation, the impact of covariate adjustment is further explored. Unless tight control over the analysis plans is established in advance, covariate adjustment can lead to seriously misleading inferences. Illustrations from the clinical trials literature are provided.;,citation_author=M. L. Beach;,citation_author=P. Meier;,citation_publication_date=1989-12;,citation_cover_date=1989-12;,citation_year=1989;,citation_issue=4 Suppl;,citation_doi=10.1016/0197-2456(89)90055-x;,citation_issn=0197-2456;,citation_pmid=2605965;,citation_volume=10;,citation_language=en-US;,citation_journal_title=Controlled Clinical Trials;">
<meta name="citation_reference" content="citation_title=Power failure: Why small sample size undermines the reliability of neuroscience;,citation_abstract=A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.;,citation_author=Katherine S. Button;,citation_author=John P. A. Ioannidis;,citation_author=Claire Mokrysz;,citation_author=Brian A. Nosek;,citation_author=Jonathan Flint;,citation_author=Emma S. J. Robinson;,citation_author=Marcus R. Munafò;,citation_publication_date=2013-05;,citation_cover_date=2013-05;,citation_year=2013;,citation_issue=5;,citation_doi=10.1038/nrn3475;,citation_issn=1471-003X;,citation_volume=14;,citation_language=en-US;,citation_journal_title=Nature Reviews Neuroscience;">
<meta name="citation_reference" content="citation_title=Center for high throughput computing;,citation_author=Center for High Throughput Computing;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_doi=10.21231/GNT1-HW21;,citation_publisher=Center for High Throughput Computing;">
<meta name="citation_reference" content="citation_title=A power primer.;,citation_abstract=One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product-moment correlation, (3) the difference between independent r s, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation.;,citation_author=Jacob Cohen;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_issue=1;,citation_volume=112;,citation_journal_title=Psychological Bulletin;">
<meta name="citation_reference" content="citation_title=The statistical power of abnormal-social psychological research: A review;,citation_author=Jacob Cohen;,citation_publication_date=1962;,citation_cover_date=1962;,citation_year=1962;,citation_issue=3;,citation_doi=10.1037/h0045186;,citation_issn=0096-851X;,citation_volume=65;,citation_journal_title=The Journal of Abnormal and Social Psychology;,citation_publisher=American Psychological Association;">
<meta name="citation_reference" content="citation_title=Statistical Power Analysis for the Behavioral Sciences;,citation_abstract=Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes: * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of &amp;amp;amp;quot;qualifying&amp;quot; dependent variables and; * expanded power and sample size tables for multiple regression/correlation.;,citation_author=Jacob Cohen;,citation_publication_date=1988-07;,citation_cover_date=1988-07;,citation_year=1988;,citation_isbn=978-0-8058-0283-2;,citation_language=en-US;">
<meta name="citation_reference" content="citation_title=Bias, precision and statistical power of analysis of covariance in the analysis of randomized trials with baseline imbalance: A simulation study;,citation_abstract=Analysis of variance (ANOVA), change-score analysis (CSA) and analysis of covariance (ANCOVA) respond differently to baseline imbalance in randomized controlled trials. However, no empirical studies appear to have quantified the differential bias and precision of estimates derived from these methods of analysis, and their relative statistical power, in relation to combinations of levels of key trial characteristics. This simulation study therefore examined the relative bias, precision and statistical power of these three analyses using simulated trial data.;,citation_author=Bolaji E. Egbewale;,citation_author=Martyn Lewis;,citation_author=Julius Sim;,citation_publication_date=2014-04;,citation_cover_date=2014-04;,citation_year=2014;,citation_issue=1;,citation_doi=10.1186/1471-2288-14-49;,citation_issn=1471-2288;,citation_volume=14;,citation_language=en-US;,citation_journal_title=BMC Medical Research Methodology;">
<meta name="citation_reference" content="citation_title=Covariate adjustment in randomized controlled trials with dichotomous outcomes increases statistical power and reduces sample size requirements;,citation_abstract=Objective Randomized controlled trials (RCTs) with dichotomous outcomes may be analyzed with or without adjustment for baseline characteristics (covariates). We studied type I error, power, and potential reduction in sample size with several covariate adjustment strategies. Study Design and Setting Logistic regression analysis was applied to simulated data sets (n=360) with different treatment effects, covariate effects, outcome incidences, and covariate prevalences. Treatment effects were estimated with or without adjustment for a single dichotomous covariate. Strategies included always adjusting for the covariate (“prespecified”), or only when the covariate was predictive or imbalanced. Results We found that the type I error was generally at the nominal level. The power was highest with prespecified adjustment. The potential reduction in sample size was higher with stronger covariate effects (from 3 to 46%, at 50% outcome incidence and covariate prevalence) and independent of the treatment effect. At lower outcome incidences and/or covariate prevalences, the reduction was lower. Conclusion We conclude that adjustment for a predictive baseline characteristic may lead to a potentially important increase in power of analyses of treatment effect. Adjusted analysis should, hence, be considered more often for RCTs with dichotomous outcomes.;,citation_author=Adrián V Hernández;,citation_author=Ewout W Steyerberg;,citation_author=J. Dik F Habbema;,citation_publication_date=2004-05;,citation_cover_date=2004-05;,citation_year=2004;,citation_issue=5;,citation_doi=10.1016/j.jclinepi.2003.09.014;,citation_issn=0895-4356;,citation_volume=57;,citation_journal_title=Journal of Clinical Epidemiology;">
<meta name="citation_reference" content="citation_title=Randomized Controlled Trials With Time-to-Event Outcomes: How Much Does Prespecified Covariate Adjustment Increase Power?;,citation_abstract=Purpose We evaluated the effects of various strategies of covariate adjustment on type I error, power, and potential reduction in sample size in randomized controlled trials (RCTs) with time-to-event outcomes. Methods We used Cox models in simulated data sets with different treatment effects (hazard ratios [HRs] = 1, 1.4, and 1.7), covariate effects (HRs = 1, 2, and 5), covariate prevalences (10% and 50%), and censoring levels (no, low, and high). Treatment and a single covariate were dichotomous. We examined the sample size that gives the same power as an unadjusted analysis for three strategies: prespecified, significant predictive, and significant imbalance. Results Type I error generally was at the nominal level. The power to detect a true treatment effect was greater with adjusted than unadjusted analyses, especially with prespecified and significant-predictive strategies. Potential reductions in sample size with a covariate HR between 2 and 5 were between 15% and 44% (covariate prevalence 50%) and between 4% and 12% (covariate prevalence 10%). The significant-imbalance strategy yielded small reductions. The reduction was greater with stronger covariate effects, but was independent of treatment effect, sample size, and censoring level. Conclusions Adjustment for one predictive baseline characteristic yields greater power to detect a true treatment effect than unadjusted analysis, without inflation of type I error and with potentially moderate reductions in sample size. Analysis of RCTs with time-to-event outcomes should adjust for predictive covariates.;,citation_author=Adrián V. Hernández;,citation_author=Marinus J. C. Eijkemans;,citation_author=Ewout W. Steyerberg;,citation_publication_date=2006-01;,citation_cover_date=2006-01;,citation_year=2006;,citation_issue=1;,citation_doi=10.1016/j.annepidem.2005.09.007;,citation_issn=1047-2797;,citation_volume=16;,citation_journal_title=Annals of Epidemiology;">
<meta name="citation_reference" content="citation_title=Why most published research findings are false;,citation_abstract=There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.;,citation_author=John P. A. Ioannidis;,citation_publication_date=2005-08;,citation_cover_date=2005-08;,citation_year=2005;,citation_issue=8;,citation_doi=10.1371/journal.pmed.0020124;,citation_issn=1549-1676;,citation_pmid=16060722;,citation_volume=2;,citation_language=en-US;,citation_journal_title=PLoS medicine;">
<meta name="citation_reference" content="citation_title=The risks and rewards of covariate adjustment in randomized trials: An assessment of 12 outcomes from 8 studies;,citation_abstract=Background Adjustment for prognostic covariates can lead to increased power in the analysis of randomized trials. However, adjusted analyses are not often performed in practice. Methods We used simulation to examine the impact of covariate adjustment on 12 outcomes from 8 studies across a range of therapeutic areas. We assessed (1) how large an increase in power can be expected in practice; and (2) the impact of adjustment for covariates that are not prognostic. Results Adjustment for known prognostic covariates led to large increases in power for most outcomes. When power was set to 80% based on an unadjusted analysis, covariate adjustment led to a median increase in power to 92.6% across the 12 outcomes (range 80.6 to 99.4%). Power was increased to over 85% for 8 of 12 outcomes, and to over 95% for 5 of 12 outcomes. Conversely, the largest decrease in power from adjustment for covariates that were not prognostic was from 80% to 78.5%. Conclusions Adjustment for known prognostic covariates can lead to substantial increases in power, and should be routinely incorporated into the analysis of randomized trials. The potential benefits of adjusting for a small number of possibly prognostic covariates in trials with moderate or large sample sizes far outweigh the risks of doing so, and so should also be considered.;,citation_author=Brennan C Kahan;,citation_author=Vipul Jairath;,citation_author=Caroline J Doré;,citation_author=Tim P Morris;,citation_publication_date=2014-04;,citation_cover_date=2014-04;,citation_year=2014;,citation_doi=10.1186/1745-6215-15-139;,citation_issn=1745-6215;,citation_pmid=24755011;,citation_volume=15;,citation_journal_title=Trials;">
<meta name="citation_reference" content="citation_title=Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: Current practiceand problems;,citation_abstract=Clinical trial investigators often record a great deal of baseline data on each patient at randomization. When reporting the trial’s findings such baseline data can be used for (i) subgroup analyses which explore whether there is evidence that the treatment difference depends on certain patient characteristics, (ii) covariate-adjusted analyses which aim to refine the analysis of the overall treatment difference by taking account of the fact that some baseline characteristics are related to outcome and may be unbalanced between treatment groups, and (iii) baseline comparisons which compare the baseline characteristics of patients in each treatment group for any possible (unlucky) differences. This paper examines how these issues are currently tackled in the medical journals, based on a recent survey of 50 trial reports in four major journals. The statistical ramifications are explored, major problems are highlighted and recommendations for future practice are proposed. Key issues include: the overuse and overinterpretation of subgroup analyses; the underuse of appropriate statistical tests for interaction; inconsistencies in the use of covariate-adjustment; the lack of clear guidelines on covariate selection; the overuse of baseline comparisons in some studies; the misuses of significance tests for baseline comparability, and the need for trials to have a predefined statistical analysis plan for all these uses of baseline data. Copyright © 2002 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Stuart J. Pocock;,citation_author=Susan E. Assmann;,citation_author=Laura E. Enos;,citation_author=Linda E. Kasten;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_issue=19;,citation_doi=10.1002/sim.1296;,citation_issn=1097-0258;,citation_volume=21;,citation_language=en-US;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=Post-hoc selection of covariates in randomized experiments;,citation_abstract=Monte Carlo methods are used to compere a number of adaptive strategies for deciding which of several covariates to incorporate into the analysis of a randomized experiment.Sixteen selection strategies in three categories are considered: 1)select covariates correlated with the response, 2)select covariates with means differing across groups, and 3)select covariates with means differing across groups that are also correlated with the response. The criteria examined are the type I error rate of the test for equality of adjusted group means and the variance of the estimated treatment effect. These strategies can result in either inflated or deflated type I errors, depending on the method and the population parameters. The adaptive methods in the first category some times yieldpoint estimates of the treatment effect more precise than estimators derive dusing either all or none of the covariates.;,citation_author=Mark D. Schelchter;,citation_author=Alan B. Forsythe;,citation_publication_date=1985-01;,citation_cover_date=1985-01;,citation_year=1985;,citation_issue=3;,citation_doi=10.1080/03610928508828942;,citation_issn=0361-0926;,citation_volume=14;,citation_journal_title=Communications in Statistics - Theory and Methods;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant;,citation_abstract=In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings ($\leq$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.;,citation_author=Joseph P. Simmons;,citation_author=Leif D. Nelson;,citation_author=Uri Simonsohn;,citation_publication_date=2011-11;,citation_cover_date=2011-11;,citation_year=2011;,citation_issue=11;,citation_doi=10.1177/0956797611417632;,citation_issn=1467-9280;,citation_pmid=22006061;,citation_volume=22;,citation_language=en-US;,citation_journal_title=Psychological Science;">
<meta name="citation_reference" content="citation_title=P-curve: A key to the file-drawer.;,citation_abstract=Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that “work,” readers must ask, “Are these effects true, or do they merely reflect selective reporting?” We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps &amp;amp;amp;lt; .05). Because only true effects are expected to generate right-skewed p-curves—containing more low (.01s) than high (.04s) significant p values—only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycInfo Database Record (c) 2025 APA, all rights reserved);,citation_author=Uri Simonsohn;,citation_author=Leif D. Nelson;,citation_author=Joseph P. Simmons;,citation_publication_date=2014-04;,citation_cover_date=2014-04;,citation_year=2014;,citation_issue=2;,citation_doi=10.1037/a0033242;,citation_issn=1939-2222;,citation_volume=143;,citation_language=en-US;,citation_journal_title=Journal of Experimental Psychology: General;,citation_publisher=American Psychological Association;">
<meta name="citation_reference" content="citation_title=Covariate adjustment had similar benefits in small and large randomized controlled trials;,citation_abstract=Objectives Covariate adjustment is a standard statistical approach in the analysis of randomized controlled trials. We aimed to explore whether the benefit of covariate adjustment on statistical significance and power differed between small and large trials, where chance imbalance in prognostic factors necessarily differs. Study Design and Setting We studied two large trial data sets [Global Use of Strategies to Open Occluded Coronary Arteries (GUSTO-I), N&nbsp;=&nbsp;30,510 and International Stroke Trial (IST), N&nbsp;=&nbsp;18,372] repeatedly drawing random samples (500,000 times) of sizes 300 and 5,000 per arm and simulated each primary outcome using the control arms. We empirically determined the treatment effects required to fix power at 80% for all unadjusted analyses and calculated the joint probabilities in the discordant cells when cross-classifying adjusted and unadjusted results from logistic regression models (ie, P&nbsp;$&amp;amp;amp;lt;~$0.05 vs. P&nbsp;$\geq~$0.05). Results The power gained from an adjusted analysis for small and large samples was between 5% and 6%. Similar proportions of discordance were noted irrespective of the sample size in both the GUSTO-I and the IST data sets. Conclusion The proportions of change in statistical significance from covariate adjustment of strongly prognostic characteristics were the same for small and large trials with similar gains in statistical power. Covariate adjustment is equally recommendable in small and large trials.;,citation_author=Douglas D. Thompson;,citation_author=Hester F. Lingsma;,citation_author=William N. Whiteley;,citation_author=Gordon D. Murray;,citation_author=Ewout W. Steyerberg;,citation_publication_date=2015-09;,citation_cover_date=2015-09;,citation_year=2015;,citation_issue=9;,citation_doi=10.1016/j.jclinepi.2014.11.001;,citation_issn=0895-4356;,citation_volume=68;,citation_journal_title=Journal of Clinical Epidemiology;">
<meta name="citation_reference" content="citation_title=P-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results;,citation_author=Uri Simonsohn;,citation_author=Leif D. Nelson;,citation_author=Joseph P. Simmons;,citation_publication_date=2014-11;,citation_cover_date=2014-11;,citation_year=2014;,citation_issue=6;,citation_doi=10.1177/1745691614553988;,citation_volume=9;,citation_journal_title=Perspectives on Psychological Science;">
<meta name="citation_reference" content="citation_title=ANCOVA versus change from baseline had more power in randomized studies and more bias in nonrandomized studies;,citation_abstract=Background and Objective: For inferring a treatment effect from the difference between a treated and untreated group on a quantitative outcome measured before and after treatment, current methods are analysis of covariance (ANCOVA) of the outcome with the baseline as covariate, and analysis of variance (ANOVA) of change from baseline. This article compares both methods on power and bias, for randomized and nonrandomized studies. Methods: The methods are compared by writing both as a regression model and as a repeated measures model, and are applied to a nonrandomized study of preventing depression. Results: In randomized studies both methods are unbiased, but ANCOVA has more power. If treatment assignment is based on the baseline, only ANCOVA is unbiased. In nonrandomized studies with preexisting groups differing at baseline, the two methods cannot both be unbiased, and may contradict each other. In the study of depression, ANCOVA suggests absence, but ANOVA of change suggests presence, of a treatment effect. The methods differ because ANCOVA assumes absence of a baseline difference. Conclusion: In randomized studies and studies with treatment assignment depending on the baseline, ANCOVA must be used. In nonrandomized studies of preexisting groups, ANOVA of change seems less biased than ANCOVA, but two control groups and two baseline measurements are recommended.;,citation_author=Gerard Van Breukelen;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_volume=59;,citation_journal_title=Journal of Clinical Epidemiology;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Evaluating Methods for Covariate Selection in Experimental Designs</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Kendra Wyant </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Department of Psychology, University of Wisconsin-Madison
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Lauren Khoury </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Department of Psychology, University of Wisconsin-Madison
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Markus Brauer </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Department of Psychology, University of Wisconsin-Madison
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">John J. Curtin <a href="mailto:jjcurtin@wisc.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Department of Psychology, University of Wisconsin-Madison
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">January 9, 2026</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>Abstract of paper goes here and can span several lines.</p>
      </div>
    </div>


    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#covariate-selection-methods" id="toc-covariate-selection-methods" class="nav-link" data-scroll-target="#covariate-selection-methods">Covariate Selection Methods</a></li>
  <li><a href="#research-contexts" id="toc-research-contexts" class="nav-link" data-scroll-target="#research-contexts">Research Contexts</a></li>
  <li><a href="#data-analytic-plan" id="toc-data-analytic-plan" class="nav-link" data-scroll-target="#data-analytic-plan">Data Analytic Plan</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#type-i-error" id="toc-type-i-error" class="nav-link" data-scroll-target="#type-i-error">Type I Error</a></li>
  <li><a href="#type-ii-error" id="toc-type-ii-error" class="nav-link" data-scroll-target="#type-ii-error">Type II Error</a></li>
  <li><a href="#parameter-estimates" id="toc-parameter-estimates" class="nav-link" data-scroll-target="#parameter-estimates">Parameter Estimates</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="notebooks/mak_figures-preview.html"><i class="bi bi-journal-code"></i>Make Figures for Main Manuscript</a></li><li><a href="notebooks/mak_tables-preview.html"><i class="bi bi-journal-code"></i>Make Tables for Main Manuscript</a></li><li><a href="notebooks/supplement-preview.html"><i class="bi bi-journal-code"></i>Supplemental Material</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<!-- Outlets:
Psychological Science (first choice)
- 2,000 word limit for intro/discussion/notes/acknowledgements/appendices
- No word count for Methods and Results, but they encourage no more than 2,500 words 
- No figure/table limit
- 40 reference limit
- Open access option ($1000 extra)


Plos One
- No word count or figure limit
- Open access journal ($2000 - $3000)
-->
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Research in Psychology is often uses experimental manipulations as a means of establishing causal relationships between a focal independent variable (IV) and a psychological outcome. For example, participants may be randomly assigned to receive a moderate dose of alcohol or placebo to evaluate the effect of alcohol consumption on response to a laboratory stressor (INSERT A COUPLE OF REFS TO OUR STUDIES). [Markus, can you provide a simple second example that focuses on an IV manipulation that is social psych relevant?)</p>
<p>A valid test of the effect of an IV requires adequate statistical power (i.e., low probability of a type II error) <span class="citation" data-cites="cohenStatisticalPowerAnalysis1988 cohenPowerPrimer1992">(<a href="#ref-cohenStatisticalPowerAnalysis1988" role="doc-biblioref">Cohen 1988</a>, <a href="#ref-cohenPowerPrimer1992" role="doc-biblioref">1992</a>)</span>. By definition, low statistical power is associated with a high probability of a type II error (i.e., failure to reject a false null hypothesis). Significant effects detected by studies with low power often do not replicate due to their low positive predictive value. Furthermore, estimates of the effect size for an IV may be positively biased when derived from studies with low power. For all these reasons, methods to increase statistical power are critically important for a robust scientific literature <span class="citation" data-cites="cohenStatisticalPowerAbnormalsocial1962 buttonPowerFailureWhy2013">(<a href="#ref-cohenStatisticalPowerAbnormalsocial1962" role="doc-biblioref">Cohen 1962</a>; <a href="#ref-buttonPowerFailureWhy2013" role="doc-biblioref">Button et al. 2013</a>)</span>.</p>
<p>The use of covariates that are measured at baseline (i.e., prior to random assignment/manipulation of the focal IV) has been shown to increase statistical power and reduce Type II errors by accounting for unexplained variance in the outcome that would otherwise be treated as noise in the analysis <span class="citation" data-cites="thompsonCovariateAdjustmentHad2015 hernandezCovariateAdjustmentRandomized2004 hernandezRandomizedControlledTrials2006 kahanRisksRewardsCovariate2014 vanbreukelenANCOVAChangeBaseline2006 egbewaleBiasPrecisionStatistical2014">(<a href="#ref-thompsonCovariateAdjustmentHad2015" role="doc-biblioref">Thompson et al. 2015</a>; <a href="#ref-hernandezCovariateAdjustmentRandomized2004" role="doc-biblioref">Adrián V. Hernández, Steyerberg, and Habbema 2004</a>; <a href="#ref-hernandezRandomizedControlledTrials2006" role="doc-biblioref">Adrián V. Hernández, Eijkemans, and Steyerberg 2006</a>; <a href="#ref-kahanRisksRewardsCovariate2014" role="doc-biblioref">Kahan et al. 2014</a>; <a href="#ref-vanbreukelenANCOVAChangeBaseline2006" role="doc-biblioref">Van Breukelen 2006</a>; <a href="#ref-egbewaleBiasPrecisionStatistical2014" role="doc-biblioref">Egbewale, Lewis, and Sim 2014</a>)</span>. For example, in the previously described test of the effect of alcohol on stress response, baseline measures of both trait anxiety and fearfulness could be included as covariates. These measures would be expected to correlate with laboratory stress response and therefore reduce variance in stress response that is unrelated to the manipulation to increase statistical power.</p>
<p>However, selection of the optimal covariates to include to increase statistical power is not straightforward. Historically, researchers often iterated over the set of possible covariates and included only those covariates that lowered the p-value for the test of their focal IV. Today, we recognize that this approach, known as p-hacking <span class="citation" data-cites="simonsohnPcurveKeyFiledrawer2014">(<a href="#ref-simonsohnPcurveKeyFiledrawer2014" role="doc-biblioref">Simonsohn, Nelson, and Simmons 2014</a>)</span>, seriously undermines the statistical validity of the analyses by substantially increasing the probability of type I errors (i.e., rejecting the null hypothesis when it is not false <span class="citation" data-cites="simmonsFalsepositivePsychologyUndisclosed2011 ioannidisWhyMostPublished2005 schelchterPosthocSelectionCovariates1985 beachChoosingCovariatesAnalysis1989">(<a href="#ref-simmonsFalsepositivePsychologyUndisclosed2011" role="doc-biblioref">Simmons, Nelson, and Simonsohn 2011</a>; <a href="#ref-ioannidisWhyMostPublished2005" role="doc-biblioref">Ioannidis 2005</a>; <a href="#ref-schelchterPosthocSelectionCovariates1985" role="doc-biblioref">Schelchter and Forsythe 1985</a>; <a href="#ref-beachChoosingCovariatesAnalysis1989" role="doc-biblioref">Beach and Meier 1989</a>)</span>). Statistically valid analyses require both low probability of type II errors and control of type I errors at the specified value of alpha (e.g., 0.05).</p>
<p>Given that p-hacking compromises statistical validity, we are left with questions about appropriate covariate selection strategies that both minimize type II errors and control type I errors. A priori, the researcher could decide to avoid the use of covariates altogether with no risk to type I error control. But statistical power would likely be lost by this decision. At the other extreme, the researcher could plan to include all available covariates that are reasonably believed to account for unexplained variance in the outcome in the statistical analysis. This decision, if made a priori, would also not be expected to inflate the type I error rate. However, this approach may not yield optimal statistical power because a subset of these covariates may not manifest strong, or any, relationship with the outcome in any particular study if they have not been previously well-established as robust predictors. Furthermore, available covariates are often correlated with each other (e.g., trait anxiety and fearfulness from the previous example), which reduces their potential benefit in reducing noise relative to their cost to degrees of freedom in the analysis.</p>
<p>Given these previous options, it may be that a data-driven approach to covariate selection could improve statistical validity relative to the a priori all or no covariate approaches. An optimal data-driven covariate selection method may be able to select the covariates that provide the greatest increase in power (e.g., covariates highly correlated with the outcome <span class="citation" data-cites="kahanRisksRewardsCovariate2014">(<a href="#ref-kahanRisksRewardsCovariate2014" role="doc-biblioref">Kahan et al. 2014</a>)</span>) with the lowest cost to degrees of freedom (e.g., by including covariates that contribute to unique reductions in variance). Of course, spastically valid data-driven approaches must also not nominally inflate the Type I error rate, as occurs with <em>p</em>-hacking.</p>
<p>In this study, we use simulation methods to evaluate six data-driven approaches to select an optimal set of covariates when testing the effect of a focal manipulated IV. For each approach, we select covariates that manifest significant relationships with the outcome variable in covariate screening models that are used to identify which covariates to include in the subsequent analysis that tests the focal IV. Across these screening models, we consider three modeling approaches (i.e., bivariate regression models that screen each covariate in its own model, multiple regression models that screen all covariates in a single model, and LASSO models that screen all covariates in a single model). Across these modeling approaches, we either include or exclude the focal IV in the screening models for a total of six data-driven approaches. We benchmark these data-driven approaches against the a priori “all covariates” and “no covariates” approaches as well as the p-hacking approach. For all approaches we quantify the probability of type I and type II errors across a variety of research contexts that vary by sample size, number of available covariates, strength of covariate effects, effect size for the IV and other relevant context characteristics. We use results from these simulations to provide clear and accessible guidance about how best to select an optimal set of covariates to increase statistical power when testing the effect of a manipulated IV.</p>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="covariate-selection-methods" class="level2">
<h2 class="anchored" data-anchor-id="covariate-selection-methods">Covariate Selection Methods</h2>
<p>As described above, we evaluated nine approaches to select covariates to use when testing the effect of a focal dichotomous IV. linear regression models with varying levels and methods of covariate selection. Two models did not use any covariate selection: a linear regression model that used no covariates and a linear regression model that used all available covariates. One model used a statistically invalid method of selecting covariates based on whether they lower the regression p-value (i.e., p-hacking). The remaining six models used three systematic covariate selection methods, selection based on a single covariate linear model, all covariates linear model, and all covariates LASSO, controlling for the focal manipulation (i.e., <em>X</em>) and without controlling for <em>X</em>. A summary of the nine models is presented in <a href="#tbl-methods" class="quarto-xref">Table&nbsp;1</a>.<!--how select?   Need to make distinction that focuses on covs not X--> <!--terminology:  not models.   Methods, procedures?--> <!--not correlation because that cant generalize to include x--> <!--all are selection methods.   But some are using data to select--></p>
<div class="quarto-embed-nb-cell">
<div class="cell">
<div id="tbl-methods" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: The nine linear regression models and their definition for covariate selection.
</figcaption>
<div aria-describedby="tbl-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<table class="lightable-classic do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Method</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">No covariates</td>
<td style="text-align: left;">Y is regressed on X without any covariates.</td>
</tr>
<tr class="even">
<td style="text-align: left;">All covariates</td>
<td style="text-align: left;">All available covariates are included in the regression model.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">p-hacking</td>
<td style="text-align: left;">Unsystematically adding covariates based on whether they lower the p-value of the main effect of X on Y.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Single covariate linear model without X</td>
<td style="text-align: left;">A linear model that regresses Y on a single covariate. Covariates are considered one at a time and included in the final model if they yield a significant effect on Y (p</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Single covariate linear model with X</td>
<td style="text-align: left;">A linear model that regresses Y on a single covariate and X. Covariates are considered one at a time and included in the final model if they yield a significant effect on Y (p</td>
</tr>
<tr class="even">
<td style="text-align: left;">All covariates linear model without X</td>
<td style="text-align: left;">A full linear model that regresses Y on all available covariates. Covariates that have a statistically significant effect on Y (p</td>
</tr>
<tr class="odd">
<td style="text-align: left;">All covariates linear model with X</td>
<td style="text-align: left;">A full linear model that regresses Y on all available covariates and X. Covariates that have a statistically significant effect on Y (p</td>
</tr>
<tr class="even">
<td style="text-align: left;">All covariates LASSO without X</td>
<td style="text-align: left;">A linear model that regresses Y on all available covariates and applies a penalty to shrink coefficients for less important covariates, potentially dropping them altogether (i.e., coefficient of 0). Covariates with non-zero coefficients were retained.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">All covariates LASSO with X</td>
<td style="text-align: left;">A linear model that regresses Y on all available covariates and applies a penalty to shrink coefficients for less important covariates. We assigned a 0 penalty to X to retain it in the model. Covariates with non-zero coefficients when controlling for X were retained.</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="research-contexts" class="level2">
<h2 class="anchored" data-anchor-id="research-contexts">Research Contexts</h2>
<p>We manipulated several characteristics designed to mimic varying research contexts that psychology researchers might encounter in their studies. We crossed all levels of each characteristic to create a total of 540 unique research contexts to allow results from our simulations to generalize across the diversity of contexts within which psychology studies are situated. A summary of these contexts is presented in <a href="#tbl-dictionary" class="quarto-xref">Table&nbsp;2</a>. The variables include:</p>
<ol type="1">
<li><p>The true population parameter for the focal IV. We selected values that represent null (<span class="math inline">\(b_{iv} = 0\)</span>), small (<span class="math inline">\(b_{iv} = 0.2\)</span>) and medium (<span class="math inline">\(b_{iv} = 0.5\)</span>) effects for the IV. Given that the variance of the outcome variable was set to 1 in all simulations, these parameter values correspond to null, small, and medium Cohen’s d effect sizes (INSERT COHEN 1991 ref).</p></li>
<li><p>The sample size. We chose values for the number of observations that pertain to common sample sizes in experimental research: 50, 100, 150, 200, 300, and 400 observations.</p></li>
<li><p>The number of covariates available. We selected a wide range of possible scenarios: 4, 8, 12, 16, or 20 available covariates.</p></li>
<li><p>The percentage of covariates with non-zero relationships with the outcome. We simulated research contexts where 25%, 50% or 75% of the available covariates were related to the outcome to represent realities where a researcher has many available covariates, but only some may be useful given their relationship with the outcome.</p></li>
<li><p>The strength of the relationship between the covariates and Y. All non-zero covariates (see characteristic 3) were given a moderate or large relationship to the outcome (i.e., correlations of 0.3 and 0.5), respectively. Since these covariates are all correlated with the outcome, it is likely they are also correlated with each other. Therefore, we assigned a moderate 0.3 correlation for all relationships among good covariates. <!--combine with 4 at least in the description here--></p></li>
</ol>
<div class="quarto-embed-nb-cell">
<div class="cell">
<div id="tbl-dictionary" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dictionary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Research context variables and values
</figcaption>
<div aria-describedby="tbl-dictionary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<table class="table table-striped table-hover do-not-create-environment cell caption-top table-sm small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Research Context Variable</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left; width: 15em;">The population parameter for X</td>
<td style="text-align: left; width: 10em;">0, 0.2, 0.5</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 15em;">The number of observations in the sample</td>
<td style="text-align: left; width: 10em;">50, 100, 150, 200, 300, 400</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 15em;">The number of covariates</td>
<td style="text-align: left; width: 10em;">4, 8, 12, 16, 20</td>
</tr>
<tr class="even">
<td style="text-align: left; width: 15em;">The proportion of "good" covariates</td>
<td style="text-align: left; width: 10em;">0.25, 0.50, 0.75</td>
</tr>
<tr class="odd">
<td style="text-align: left; width: 15em;">The correlation between Y and good covariates</td>
<td style="text-align: left; width: 10em;">0.3, 0.5</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="data-analytic-plan" class="level2">
<h2 class="anchored" data-anchor-id="data-analytic-plan">Data Analytic Plan</h2>
<p>All data analyses were done in R (version 4.4.2). <!--not sure about version.  Maybe also cite RStudio and tidyverse-->Simulations were run using high-throughput computing resources provided by the University of Wisconsin-Madison Center for High Throughput Computing <span class="citation" data-cites="chtc">(<a href="#ref-chtc" role="doc-biblioref">Center for High Throughput Computing 2006</a>)</span>.</p>
<p>We ran 40,000 simulations for each research context. Within each simulation we generated a unique dataset that consisted of a dichotomous focal variable (<em>X</em>), varying numbers of quantitative covariates, where a subset are correlated with each other and with <em>Y</em> (see Research Contexts section), and a quantitative outcome (<em>Y</em>) calculated by adding the <em>X</em> variable multiplied by the effect size (i.e., population parameter) to the <em>Y</em> generated from the correlation matrix with the covariates. We fit models from our nine methods on each simulated dataset. From each model we saved out the parameter estimate, standard error, and p-value for <em>X</em>. <!--edit above a bit for clarity--> <!--true and false positve rates are confusing.  Think of X but you mean cov as good.  Maybe we dont even need those analyses?--><!--KW: John, I removed sentence about true and false positive rates since we decided not to include this in manuscript--></p>
<!--show code to generate Y in paper??-->
<p>For research contexts where the population parameter for <em>X</em> is 0 (i.e., <em>X</em> has no effect on <em>Y</em>), we report the Type I error rate for each method both across and within research contexts. For research contexts where the population parameter for <em>X</em> is 0.2 or 0.5 (i.e., <em>X</em> has an effect on <em>Y</em>), we report the Type II error rate for each method across and within research contexts. We also provide sampling distributions of the parameter estimate for <em>X</em> across research contexts for each method, separately by true effect size (0, 0.2, and 0.5). Detailed tables of range and average error rates for each research context and true and false positive rates for covariates are available in the supplement.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="type-i-error" class="level2">
<h2 class="anchored" data-anchor-id="type-i-error">Type I Error</h2>
<p>Since this is simulation study, we were able to set the true population parameter for <em>X</em> to be zero (i.e., <span class="math inline">\(b_x = 0\)</span>). Therefore, any significant result found was a Type I error. <a href="#fig-bar-1" class="quarto-xref">Figure&nbsp;1</a> shows the average Type I error rate across all research contexts for each method. The p-hacking method for selecting covariates, unsurprisingly led to a highly inflated Type I error rate, consistent with the extant research published in the last several years on the connection between researcher degrees of freedom and false positives. Most other methods remained around the expected .05 threshold. There was some elevation in Type I errors for methods that controlled for <em>X</em> compared to those, however, it is not clear that these differences are substantial.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-bar-1" class="cell" data-fig-width="8.75">
<div class="cell-output cell-output-display">
<div id="fig-bar-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bar-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/notebooks-mak_figures-fig-bar-1-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Type I Error by Method. Average type I error rate across all research contexts is displayed for each method separately by color. A priori selection methods are on the left, data-driven selection methods are on the right. Striped shading indicates data-driven selection methods that do not control for X."><img src="index_files/figure-html/notebooks-mak_figures-fig-bar-1-output-1.png" width="840" height="480" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bar-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Type I Error by Method. Average type I error rate across all research contexts is displayed for each method separately by color. A priori selection methods are on the left, data-driven selection methods are on the right. Striped shading indicates data-driven selection methods that do not control for X.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<!--make point that type 1 error is .05 for all and no covariates approach before p-hacking -->
<p>We assessed Type I error by individual research context and found several patterns (<a href="#fig-type1-panel" class="quarto-xref">Figure&nbsp;2</a>). First, small sample sizes are more susceptible to inflated Type I error rates when using covariate selection methods that control for <em>X</em>. As sample sizes get larger (i.e., <em>n</em> = 300), the methods become comparable. Second, as the number of available covariates to select from increases, Type I error rate increases for covariate selection methods that control for <em>X</em>, such as all covariates linear model with <em>X</em> and all covariates LASSO with <em>X</em>, and to a lesser degree single covariate linear model. Third, there appeared to be no definitive pattern in Type I error rate as the proportion of good covariates increased<!--maybe error of LASSO decreases slightly and error of full linear model increases?-->. Similarly, we did not see changes in Type I error rate as function of the correlation strength between covariates and <em>Y</em>.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-type1-panel" class="cell" data-fig-height="6" data-fig-width="8">
<div class="cell-output cell-output-display">
<div id="fig-type1-panel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-type1-panel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/notebooks-mak_figures-fig-type1-panel-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Type I error by method and research context. Plots are paneled by aspects of the research context (number of observations, number of covariates, proportion of good covariates, and Y-covariate correlation strength). Methods are displayed separately by color. Data-driven correlation selection methods that control for X are depicted as dashed lines."><img src="index_files/figure-html/notebooks-mak_figures-fig-type1-panel-output-1.png" width="768" height="576" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-type1-panel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Type I error by method and research context. Plots are paneled by aspects of the research context (number of observations, number of covariates, proportion of good covariates, and Y-covariate correlation strength). Methods are displayed separately by color. Data-driven correlation selection methods that control for X are depicted as dashed lines.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="type-ii-error" class="level2">
<h2 class="anchored" data-anchor-id="type-ii-error">Type II Error</h2>
<p>We simulated datasets with two possible population parameters for the effect of <em>X</em> on <em>Y</em> (<span class="math inline">\(b_x = 0.2\)</span>, <span class="math inline">\(b_x = 0.5\)</span>). Any non-significant result found indicated a Type II error. Stronger statistical methods will have lower Type II error (implying greater statistical power). Since we demonstrated that p-hacking substantially inflates Type I error, making it a statistically invalid method for covariate selection, we do not evaluate Type II Error for this method.</p>
<p><a href="#fig-bar-2" class="quarto-xref">Figure&nbsp;3</a> shows the average Type II error rate across all research contexts for each method. Using no covariates results in the highest Type II error highlighting the importance of covariates for detecting true effects. Type II error rates trended lower for covariate selection methods that controlled for <em>X</em> compared to those that did not control for <em>X</em>. <!-- what about all covariates?--></p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-bar-2" class="cell" data-fig-width="8.75">
<div class="cell-output cell-output-display">
<div id="fig-bar-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bar-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/notebooks-mak_figures-fig-bar-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Type II Error by Method. Average type II error rate across all research settings is displayed for each method separately by color. A priori selection methods are on the left, data-driven selection methods are on the right. Striped shading indicates data-driven selection methods that do not control for X."><img src="index_files/figure-html/notebooks-mak_figures-fig-bar-2-output-1.png" width="840" height="480" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bar-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Type II Error by Method. Average type II error rate across all research settings is displayed for each method separately by color. A priori selection methods are on the left, data-driven selection methods are on the right. Striped shading indicates data-driven selection methods that do not control for X.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We also assessed Type II error by individual research context (<a href="#fig-panel-2" class="quarto-xref">Figure&nbsp;4</a>). Across all research context, using no covariates was associated with higher Type II error. Using all available covariates or selection methods that do not control for <em>X</em> performed had higher Type II error rates compared to the other methods when sample sizes were low. This pattern was especially notable for using all covariates and an all covariates linear model without <em>X</em>. As sample size increased, the methods performed comparably with respect to Type II error. Using all covariates or an all covariates linear model without <em>X</em> for selection produced higher Type II error rates compared to other methods when there was a larger number of covariates available. The all covariates linear model without <em>X</em> also produced higher Type II error rates, compared to other methods when the proportion of good covariates was higher.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-panel-2" class="cell" data-fig-height="9" data-fig-width="7.5">
<div class="cell-output cell-output-display">
<div id="fig-panel-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-panel-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/notebooks-mak_figures-fig-panel-2-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Type II error by method and research context. Plots are paneled by aspects of the research context (number of observations, number of covariates, proportion of good covariates, and Y-covariate correlation strength, and population parameter for X). Methods are displayed separately by color. Data-driven correlation selection methods that control for X are depicted as dashed lines."><img src="index_files/figure-html/notebooks-mak_figures-fig-panel-2-output-2.png" width="720" height="864" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-panel-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Type II error by method and research context. Plots are paneled by aspects of the research context (number of observations, number of covariates, proportion of good covariates, and Y-covariate correlation strength, and population parameter for X). Methods are displayed separately by color. Data-driven correlation selection methods that control for X are depicted as dashed lines.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="parameter-estimates" class="level2">
<h2 class="anchored" data-anchor-id="parameter-estimates">Parameter Estimates</h2>
<!--table rather than distributions.  Three columns for three b.   Doesn't without X show some bias???-->
<!--remind that when assumptions met, parameter estimates are unbiased-->
<!--need to look at this and think a bit more.  Should this also be reported separately for b =0 vs. b <>0 and embedded in those sections above.  Can make the point that we want low type II, .05 type I and unbiased at the start of method or end of intro-->
<p><a href="#tbl-est" class="quarto-xref">Table&nbsp;3</a> shows the mean parameter estimate of <em>X</em> for all nine methods separately by effect size (<span class="math inline">\(b_x = 0\)</span>, <span class="math inline">\(b_x = 0.2\)</span>, <span class="math inline">\(b_x = 0.5\)</span>). All methods yield unbiased estimates when the effect size is 0. When there is a non-zero effect, p-hacking methods notably inflate the effect size. Other data-driven methods that do not control for X yield slightly biased parameter estimates. Sampling distributions of parameter estimates are available in the supplement.</p>
<div class="quarto-embed-nb-cell">
<div class="cell">
<div id="tbl-est" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-est-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Mean estimates of effect size by method and true effect size
</figcaption>
<div aria-describedby="tbl-est-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<table class="lightable-classic do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Method</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">No effect</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">d = .2</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">d = .5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A priori selection</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even" data-grouplength="2">
<td colspan="4" style="text-align: left; border-bottom: 0;"><strong></strong></td>
</tr>
<tr class="odd">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">No covariates</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.500</td>
</tr>
<tr class="even">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">All covariates</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.500</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data-driven selection</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even" data-grouplength="7">
<td colspan="4" style="text-align: left; border-bottom: 0;"><strong></strong></td>
</tr>
<tr class="odd">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">p-hacking</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.262</td>
<td style="text-align: right;">0.563</td>
</tr>
<tr class="even">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">Single covariate lm without X</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.198</td>
<td style="text-align: right;">0.495</td>
</tr>
<tr class="odd">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">Single covariate lm with X</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.500</td>
</tr>
<tr class="even">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">All covariates lm without X</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.192</td>
<td style="text-align: right;">0.479</td>
</tr>
<tr class="odd">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">All covariates lm with X</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.500</td>
</tr>
<tr class="even">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">All covariates LASSO without X</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.195</td>
<td style="text-align: right;">0.487</td>
</tr>
<tr class="odd">
<td style="text-align: left; padding-left: 2em;" data-indentlevel="1">All covariates LASSO with X</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.500</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</div>
<!--TABLE of TYPE1 and TYPE 2 by method, b and context?  In paper or supplement-->
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Our findings reinforce the cautionary warning by Simmons and colleagues <span class="citation" data-cites="simmonsFalsepositivePsychologyUndisclosed2011">(<a href="#ref-simmonsFalsepositivePsychologyUndisclosed2011" role="doc-biblioref">Simmons, Nelson, and Simonsohn 2011</a>)</span>: p-hacking severely inflates Type I error rate. Simmons et al.&nbsp;simulated an average Type I error rate of 11.7% for a treatment effect when selecting between two potential covariates (<em>N</em>=40). In our closest research context (<em>N</em>=50, 4 covariates), we observed similar rates of 10.3% (range: 8.4%-13.5%). However, our simulations suggest this estimate may gravely underestimate the magnitude of the problem of p-hacking. It is more realistic that researchers must choose from several candidate covariates. Even when the number of available covariates is modest (i.e., 8-12), our results show that Type I error rates increase to 17.6% (range: 9.4%-32.9%) across contexts. When selecting among 16-20 covariates, Type I error rates, on average, reach a 2.5-fold increase from the 4-covariate context (26.9%, range: 13.9%-46.2%).</p>
<p>When covariates were completely withheld (i.e., no covariates method), simulated models yielded an average Type I error rate of .05 and unbiased parameter estimates. However, this approach also showed a meaningfully higher Type II error rate compared to the other methods. This is not surprising. Adding covariates correlated with the outcome and uncorrelated with <em>X</em> will account for additional variance in the outcome, thus increasing power (or lowering Type II error rate). Perhaps then it is also not surprising that the research contexts with highest discrepancies in Type II error rates between the no covariates method and selection methods include situations with moderate to large sample sizes (<em>N</em>=150-400) where degrees of freedom can afford the cost of additional parameters, and in situations where there are many covariates to choose from with strong correlations to the outcome, where covariates have the most opportunity to reduce unexplained variance in the outcome.</p>
<p>In many instances, using all available covariates selected a priori to data analysis is a reasonable option. Our simulated models showed consistent average Type I error rates of .05 and unbiased parameter estimates. Additionally, on average Type II error rates were only about 2% higher than some of the best performing data-driven selection methods (i.e., all covariates LASSO with <em>X</em> and single covariate linear model with <em>X</em>). Using all available covariates is easiest because it eliminates the need for selection. However, in certain contexts, such as with small sample sizes or large numbers of covariates this difference is no longer trivial, with some data-driven selection methods yielding Type II error rates as much as 9% lower than the all covariates method. These findings are concerning in that it is not uncommon in psychological research to have 20+ covariates available. Moreover, the high cost of running experimental studies may confine researchers to smaller than ideal sample sizes. Moreover, even with an adequate sample size and modest number of covariates, it may not be wise to walk away from any power to detect an effect. <!--JC note: Anchor Type II benefit?  small but how to think about it?   N increase but not linear so specific N?--></p>
<p>Among the three data-driven selection methods our simulations showed that when the methods did not include the focal variable, <em>X</em>, in the model they produced biased parameter estimates. This bias results from the models favoring selecting covariates that are correlated with <em>X</em> in the sample despite being uncorrelated in the population. In other words, the models are accounting for variance in the outcome related to <em>X</em> by capitalizing on spurious correlations. This bias was largest for the all covariates linear model and trivial with the single covariate linear model.</p>
<p>However, data-driven selection methods that included <em>X</em> in the model produced larger Type I error inflation. <!--JC notes: 
- Anchor Type I inflation - need simulation
- Why? Flexibility of models? Seen across all methods that include X.--></p>
<p>Our findings suggest the all covariates linear model is not an optimal data-driven covariate selection method. On average it performed worse than the other data-driven methods. When no true effect existed the all covariates linear model with <em>X</em> yielded the highest Type II error compared to the other data-driven selection methods. When there was a true effect, the all covariates linear model with and without <em>X</em> showed higher Type II error rates compared to the respective single covariate linear and all covariates LASSO models, with and without <em>X</em>. Finally, the all covariates model with <em>X</em> produced the most biased parameter estimates, compared to the other data-driven selection methods, when true effects existed.</p>
<p>The absolute best method for Type II error is all covariates LASSO with <em>X</em> included in the model. LASSO is a machine learning algorithm that may be unfamiliar to some psychology researchers. As such, this data-driven selection method may feel overly complicated. We provide code in the supplement<!--need to add this if JC agrees--> to demonstrate how LASSO can be easily implemented in R for covariate selection.</p>
<p>However, the simple single covariate linear model with <em>X</em> included in the model may be an equally viable alternative to LASSO. This method performed almost as good as LASSO with regard to Type II error and had lower Type I error compared to LASSO. Moreover, this method is easy to implement as it can be run as a table of correlation or partial correlation values. Both of these methods yielded unbiased parameter estimates and are likely comparable. Researchers can weigh the trade off between Type I and Type II error and the complexity of these methods in choosing which data-driven selection method to use. And of course, this decision should be pre-registered to avoid introducing additional researcher degrees of freedom into the analyses.</p>
<p>This study demonstrated the gains in power when using covariates measured before assignment. These findings are likely still applicable to covariates uncorrelated to the focal variable and empirically related to the outcome but measured after assignment. Another use of covariates that we did not address in this study is to statistically control for confounding effects by including additional variables related to both the focal variable and the outcome. This is common practice in situations where <em>X</em> cannot be manipulated (e.g., controlling for gender when looking at the effects of parenting style on child school performance). This use of covariates, however, is complicated and covariates must be selected carefully based on theory and causal structure. Unfortunately, controlling for a confounding variable is also not as strong as people tend to believe. Controlling for a confounding variable with a covariate does not fully eliminate its influence but removes the partial effect on the outcome measurable by the model. Given the complex nature of these types of covariates, we considered them to fall outside the scope of the current paper and we plan to address them in a future simulation paper.</p>
</section>
<section id="references" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-beachChoosingCovariatesAnalysis1989" class="csl-entry" role="listitem">
Beach, M. L., and P. Meier. 1989. <span>“Choosing Covariates in the Analysis of Clinical Trials.”</span> <em>Controlled Clinical Trials</em> 10 (4 Suppl): 161S–175S. <a href="https://doi.org/10.1016/0197-2456(89)90055-x">https://doi.org/10.1016/0197-2456(89)90055-x</a>.
</div>
<div id="ref-buttonPowerFailureWhy2013" class="csl-entry" role="listitem">
Button, Katherine S., John P. A. Ioannidis, Claire Mokrysz, Brian A. Nosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R. Munafò. 2013. <span>“Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience.”</span> <em>Nature Reviews Neuroscience</em> 14 (5): 365–76. <a href="https://doi.org/10.1038/nrn3475">https://doi.org/10.1038/nrn3475</a>.
</div>
<div id="ref-chtc" class="csl-entry" role="listitem">
Center for High Throughput Computing. 2006. <span>“Center for High Throughput Computing.”</span> Center for High Throughput Computing. <a href="https://doi.org/10.21231/GNT1-HW21">https://doi.org/10.21231/GNT1-HW21</a>.
</div>
<div id="ref-cohenStatisticalPowerAbnormalsocial1962" class="csl-entry" role="listitem">
Cohen, Jacob. 1962. <span>“The Statistical Power of Abnormal-Social Psychological Research: <span>A</span> Review.”</span> <em>The Journal of Abnormal and Social Psychology</em> 65 (3): 145–53. <a href="https://doi.org/10.1037/h0045186">https://doi.org/10.1037/h0045186</a>.
</div>
<div id="ref-cohenStatisticalPowerAnalysis1988" class="csl-entry" role="listitem">
———. 1988. <em>Statistical <span>Power Analysis</span> for the <span>Behavioral Sciences</span></em>. 2 edition. Hillsdale, N.J: Routledge.
</div>
<div id="ref-cohenPowerPrimer1992" class="csl-entry" role="listitem">
———. 1992. <span>“A Power Primer.”</span> <em>Psychological Bulletin</em> 112 (1): 155–59.
</div>
<div id="ref-egbewaleBiasPrecisionStatistical2014" class="csl-entry" role="listitem">
Egbewale, Bolaji E., Martyn Lewis, and Julius Sim. 2014. <span>“Bias, Precision and Statistical Power of Analysis of Covariance in the Analysis of Randomized Trials with Baseline Imbalance: A Simulation Study.”</span> <em>BMC Medical Research Methodology</em> 14 (1): 49. <a href="https://doi.org/10.1186/1471-2288-14-49">https://doi.org/10.1186/1471-2288-14-49</a>.
</div>
<div id="ref-hernandezRandomizedControlledTrials2006" class="csl-entry" role="listitem">
Hernández, Adrián V., Marinus J. C. Eijkemans, and Ewout W. Steyerberg. 2006. <span>“Randomized <span class="nocase">Controlled Trials With Time-to-Event Outcomes</span>: <span>How Much Does Prespecified Covariate Adjustment Increase Power</span>?”</span> <em>Annals of Epidemiology</em> 16 (1): 41–48. <a href="https://doi.org/10.1016/j.annepidem.2005.09.007">https://doi.org/10.1016/j.annepidem.2005.09.007</a>.
</div>
<div id="ref-hernandezCovariateAdjustmentRandomized2004" class="csl-entry" role="listitem">
Hernández, Adrián V, Ewout W Steyerberg, and J. Dik F Habbema. 2004. <span>“Covariate Adjustment in Randomized Controlled Trials with Dichotomous Outcomes Increases Statistical Power and Reduces Sample Size Requirements.”</span> <em>Journal of Clinical Epidemiology</em> 57 (5): 454–60. <a href="https://doi.org/10.1016/j.jclinepi.2003.09.014">https://doi.org/10.1016/j.jclinepi.2003.09.014</a>.
</div>
<div id="ref-ioannidisWhyMostPublished2005" class="csl-entry" role="listitem">
Ioannidis, John P. A. 2005. <span>“Why Most Published Research Findings Are False.”</span> <em>PLoS Medicine</em> 2 (8): e124. <a href="https://doi.org/10.1371/journal.pmed.0020124">https://doi.org/10.1371/journal.pmed.0020124</a>.
</div>
<div id="ref-kahanRisksRewardsCovariate2014" class="csl-entry" role="listitem">
Kahan, Brennan C, Vipul Jairath, Caroline J Doré, and Tim P Morris. 2014. <span>“The Risks and Rewards of Covariate Adjustment in Randomized Trials: An Assessment of 12 Outcomes from 8 Studies.”</span> <em>Trials</em> 15 (April): 139. <a href="https://doi.org/10.1186/1745-6215-15-139">https://doi.org/10.1186/1745-6215-15-139</a>.
</div>
<div id="ref-schelchterPosthocSelectionCovariates1985" class="csl-entry" role="listitem">
Schelchter, Mark D., and Alan B. Forsythe. 1985. <span>“Post-Hoc Selection of Covariates in Randomized Experiments.”</span> <em>Communications in Statistics - Theory and Methods</em> 14 (3): 679–99. <a href="https://doi.org/10.1080/03610928508828942">https://doi.org/10.1080/03610928508828942</a>.
</div>
<div id="ref-simmonsFalsepositivePsychologyUndisclosed2011" class="csl-entry" role="listitem">
Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. 2011. <span>“False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.”</span> <em>Psychological Science</em> 22 (11): 1359–66. <a href="https://doi.org/10.1177/0956797611417632">https://doi.org/10.1177/0956797611417632</a>.
</div>
<div id="ref-simonsohnPcurveKeyFiledrawer2014" class="csl-entry" role="listitem">
Simonsohn, Uri, Leif D. Nelson, and Joseph P. Simmons. 2014. <span>“P-Curve: <span>A</span> Key to the File-Drawer.”</span> <em>Journal of Experimental Psychology: General</em> 143 (2): 534–47. <a href="https://doi.org/10.1037/a0033242">https://doi.org/10.1037/a0033242</a>.
</div>
<div id="ref-thompsonCovariateAdjustmentHad2015" class="csl-entry" role="listitem">
Thompson, Douglas D., Hester F. Lingsma, William N. Whiteley, Gordon D. Murray, and Ewout W. Steyerberg. 2015. <span>“Covariate Adjustment Had Similar Benefits in Small and Large Randomized Controlled Trials.”</span> <em>Journal of Clinical Epidemiology</em> 68 (9): 1068–75. <a href="https://doi.org/10.1016/j.jclinepi.2014.11.001">https://doi.org/10.1016/j.jclinepi.2014.11.001</a>.
</div>
<div id="ref-vanbreukelenANCOVAChangeBaseline2006" class="csl-entry" role="listitem">
Van Breukelen, Gerard. 2006. <span>“<span>ANCOVA</span> Versus Change from Baseline Had More Power in Randomized Studies and More Bias in Nonrandomized Studies.”</span> <em>Journal of Clinical Epidemiology</em> 59: 920–25.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>