---
title: Evaluating Methods for Covariate Selection in Experimental Designs
author:
  - name: Kendra Wyant
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Lauren Khoury
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Markus Brauer
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
abstract: |
  Abstract of paper goes here and can span several lines.
date: last-modified
bibliography: references.bib
bibliographystyle: apa
number-sections: false 
editor_options: 
  chunk_output_type: console
---

<!-- Outlets:
Psychological Science (first choice)
- 2,000 word limit for intro/discussion/notes/acknowledgements/appendices
- No word count for Methods and Results, but they encourage no more than 2,500 words 
- No figure/table limit
- 40 reference limit
- Open access option ($1000 extra)


Plos One
- No word count or figure limit
- Open access journal ($2000 - $3000)
-->
# Introduction

Research in Psychology is often uses experimental manipulations as a means of establishing causal relationships between a focal independent variable (IV) and a psychological outcome. For example, participants may be randomly assigned to receive a moderate dose of alcohol or placebo to evaluate the effect of alcohol consumption on response to a laboratory stressor (INSERT A COUPLE OF REFS TO OUR STUDIES).  [Markus, can you provide a simple second example that focuses on an IV manipulation that is social psych relevant?)

A valid test of the effect of an IV requires adequate statistical power (i.e., low probability of a type II error) [@cohenStatisticalPowerAnalysis1988; @cohenPowerPrimer1992]. By definition, low statistical power is associated with a high probability of a type II error (i.e., failure to reject a false null hypothesis).  Significant effects detected by studies with low power often do not replicate due to their low positive predictive value.  Furthermore, estimates of the effect size for an IV may be positively biased when derived from studies with low power.  For all these reasons, methods to increase statistical power are critically important for a robust scientific literature [@cohenStatisticalPowerAbnormalsocial1962; @buttonPowerFailureWhy2013]. 

The use of covariates that are measured at baseline (i.e., prior to random assignment/manipulation of the focal IV) has been shown to increase statistical power and reduce Type II errors by accounting for unexplained variance in the outcome that would otherwise be treated as noise in the analysis [@thompsonCovariateAdjustmentHad2015; @hernandezCovariateAdjustmentRandomized2004; @hernandezRandomizedControlledTrials2006; @kahanRisksRewardsCovariate2014; @vanbreukelenANCOVAChangeBaseline2006; @egbewaleBiasPrecisionStatistical2014]. For example, in the previously described test of the effect of alcohol on stress response, baseline measures of both trait anxiety and fearfulness could be included as covariates.  These measures would be expected to correlate with laboratory stress response and therefore reduce variance in stress response that is unrelated to the manipulation to increase statistical power. 

However, selection of the optimal covariates to include to increase statistical power is not straightforward. Historically, researchers often iterated over the set of possible covariates and included only those covariates that lowered the p-value for the test of their focal IV.  Today, we recognize that this approach, known as p-hacking [@simonsohnPcurveKeyFiledrawer2014], seriously undermines the statistical validity of the analyses by substantially increasing the probability of type I errors (i.e., rejecting the null hypothesis when it is not false [@simmonsFalsepositivePsychologyUndisclosed2011; @ioannidisWhyMostPublished2005; @schelchterPosthocSelectionCovariates1985; @beachChoosingCovariatesAnalysis1989]). Statistically valid analyses require both low probability of type II errors and control of type I errors at the specified value of alpha (e.g., 0.05).  

Given that p-hacking compromises statistical validity, we are left with questions about appropriate covariate selection strategies that both minimize type II errors and control type I errors.  A priori, the researcher could decide to avoid the use of covariates altogether with no risk to type I error control.  But statistical power would likely be lost by this decision.  At the other extreme, the researcher could plan to include all available covariates that are reasonably believed to account for unexplained variance in the outcome in the statistical analysis. This decision, if made a priori, would also not be expected to inflate the type I error rate. However, this approach may not yield optimal statistical power because a subset of these covariates may not manifest strong, or any, relationship with the outcome in any particular study if they have not been previously well-established as robust predictors.  Furthermore, available covariates are often correlated with each other (e.g., trait anxiety and fearfulness from the previous example), which reduces their potential benefit in reducing noise relative to their cost to degrees of freedom in the analysis.  

Given these previous options, it may be that a data-driven approach to covariate selection could improve statistical validity relative to the a priori all or no covariate approaches.  An optimal data-driven covariate selection method may be able to select the covariates that provide the greatest increase in power (e.g., covariates highly correlated with the outcome [@kahanRisksRewardsCovariate2014]) with the lowest cost to degrees of freedom (e.g., by including covariates that contribute to unique reductions in variance). Of course, spastically valid data-driven approaches must also not nominally inflate the Type I error rate, as occurs with *p*-hacking. 

In this study, we use simulation methods to evaluate six data-driven approaches to select an optimal set of covariates when testing the effect of a focal manipulated IV.  For each approach, we select covariates that manifest significant relationships with the outcome variable in covariate screening models that are used to identify which covariates to include in the subsequent analysis that tests the focal IV.  Across these screening models, we consider three modeling approaches (i.e., bivariate regression models that screen each covariate in its own model, multiple regression models that screen all covariates in a single model, and LASSO models that screen all covariates in a single model).  Across these modeling approaches, we either include or exclude the focal IV in the screening models for a total of six data-driven approaches.  We benchmark these data-driven approaches against the a priori "all covariates" and "no covariates" approaches as well as the p-hacking approach.  For all approaches we quantify the probability of type I and type II errors across a variety of research contexts that vary by sample size, number of available covariates, strength of covariate effects, effect size for the IV and other relevant context characteristics.  We use results from these simulations to provide clear and accessible guidance about how best to select an optimal set of covariates to increase statistical power when testing the effect of a manipulated IV.  


# Methods

## Covariate Selection Methods

As described above, we evaluated nine approaches to select covariates to use when testing the effect of a focal dichotomous IV.   linear regression models with varying levels and methods of covariate selection. Two models did not use any covariate selection: a linear regression model that used no covariates and a linear regression model that used all available covariates. One model used a statistically invalid method of selecting covariates based on whether they lower the regression p-value (i.e., p-hacking). The remaining six models used three systematic covariate selection methods, selection based on a single covariate linear model, all covariates linear model, and all covariates LASSO, controlling for the focal manipulation (i.e., *X*) and without controlling for *X*. A summary of the nine models is presented in @tbl-methods.<!--how select?   Need to make distinction that focuses on covs not X-->
<!--terminology:  not models.   Methods, procedures?-->
<!--not correlation because that cant generalize to include x-->
<!--all are selection methods.   But some are using data to select-->
{{< embed notebooks/mak_tables.qmd#tbl-methods >}}



## Research Contexts
We manipulated several characteristics designed to mimic varying research contexts that psychology researchers might encounter in their studies. We crossed all levels of each characteristic to create a total of 540 unique research contexts to allow results from our simulations to generalize across the diversity of contexts within which psychology studies are situated. A summary of these contexts is presented in @tbl-dictionary. The variables include:

1. The true population parameter for the focal IV. We selected values that represent null ($b_{iv} = 0$), small ($b_{iv} = 0.2$) and medium ($b_{iv} = 0.5$) effects for the IV. Given that the variance of the outcome variable was set to 1 in all simulations, these parameter values correspond to null, small, and medium Cohen's d effect sizes (INSERT COHEN 1991 ref).

2. The sample size. We chose values for the number of observations that pertain to common sample sizes in experimental research: 50, 100, 150, 200, 300, and 400 observations.

3. The number of covariates available. We selected a wide range of possible scenarios: 4, 8, 12, 16, or 20 available covariates.

4. The percentage of covariates with non-zero relationships with the outcome. We simulated research contexts where 25%, 50% or 75% of the available covariates were related to the outcome to represent realities where a researcher has many available covariates, but only some may be useful given their relationship with the outcome.

5. The strength of the relationship between the covariates and Y. All non-zero covariates (see characteristic 3) were given a moderate or large relationship to the outcome (i.e., correlations of 0.3 and 0.5), respectively. Since these covariates are all correlated with the outcome, it is likely they are also correlated with each other. Therefore, we assigned a moderate 0.3 correlation for all relationships among good covariates. 
<!--combine with 4 at least in the description here-->


{{< embed notebooks/mak_tables.qmd#tbl-dictionary >}}


## Data Analytic Plan

All data analyses were done in R (version 4.4.2). <!--not sure about version.  Maybe also cite RStudio and tidyverse-->Simulations were run using high-throughput computing resources provided by the University of Wisconsin-Madison Center for High Throughput Computing [@chtc]. 

We ran 40,000 simulations for each research context. Within each simulation we generated a unique dataset that consisted of a dichotomous focal variable (*X*), varying numbers of quantitative covariates, where a subset are correlated with each other and with *Y* (see Research Contexts section), and a quantitative outcome (*Y*) calculated by adding the *X* variable multiplied by the effect size (i.e., population parameter) to the *Y* generated from the correlation matrix with the covariates. We fit models from our nine methods on each simulated dataset. From each model we saved out the parameter estimate, standard error, and p-value for *X*. 
<!--edit above a bit for clarity-->
<!--true and false positve rates are confusing.  Think of X but you mean cov as good.  Maybe we dont even need those analyses?--><!--KW: John, I removed sentence about true and false positive rates since we decided not to include this in manuscript-->

<!--show code to generate Y in paper??-->

For research contexts where the population parameter for *X* is 0 (i.e., *X* has no effect on *Y*), we report the Type I error rate for each method both across and within research contexts. For research contexts where the population parameter for *X* is 0.2 or 0.5 (i.e., *X* has an effect on *Y*), we report the Type II error rate for each method across and within research contexts. We also provide sampling distributions of the parameter estimate for *X* across research contexts for each method, separately by true effect size (0, 0.2, and 0.5). Detailed tables of range and average error rates for each research context and true and false positive rates for covariates are available in the supplement.


# Results

## Type I Error

Since this is simulation study, we were able to set the true population parameter for *X* to be zero (i.e., $b_x = 0$). Therefore, any significant result found was a Type I error. @fig-bar-1 shows the average Type I error rate across all research contexts for each method. The p-hacking method for selecting covariates, unsurprisingly led to a highly inflated Type I error rate, consistent with the extant research published in the last several years on the connection between researcher degrees of freedom and false positives. Most other methods remained around the expected .05 threshold. There was some elevation in Type I errors for methods that controlled for *X* compared to those, however, it is not clear that these differences are substantial.

{{< embed notebooks/mak_figures.qmd#fig-bar-1 >}}

<!--make point that type 1 error is .05 for all and no covariates approach before p-hacking -->

We assessed Type I error by individual research context and found several patterns (@fig-type1-panel). First, small sample sizes are more susceptible to inflated Type I error rates when using covariate selection methods that control for *X*. As sample sizes get larger (i.e., *n* = 300), the methods become comparable. Second, as the number of available covariates to select from increases, Type I error rate increases for covariate selection methods that control for *X*, such as all covariates linear model with *X* and all covariates LASSO with *X*, and to a lesser degree single covariate linear model. Third, there appeared to be no definitive pattern in Type I error rate as the proportion of good covariates increased<!--maybe error of LASSO decreases slightly and error of full linear model increases?-->. Similarly, we did not see changes in Type I error rate as function of the correlation strength between covariates and *Y*.

{{< embed notebooks/mak_figures.qmd#fig-type1-panel >}}




## Type II Error

We simulated datasets with two possible population parameters for the effect of *X* on *Y* ($b_x = 0.2$, $b_x = 0.5$). Any non-significant result found indicated a Type II error. Stronger statistical methods will have lower Type II error (implying greater statistical power). Since we demonstrated that p-hacking substantially inflates Type I error, making it a statistically invalid method for covariate selection, we do not evaluate Type II Error for this method. 

[@fig-bar-2] shows the average Type II error rate across all research contexts for each method. Using no covariates results in the highest Type II error highlighting the importance of covariates for detecting true effects. Type II error rates trended lower for covariate selection methods that controlled for *X* compared to those that did not control for *X*.
<!-- what about all covariates?-->
{{< embed notebooks/mak_figures.qmd#fig-bar-2 >}}

We also assessed Type II error by individual research context (@fig-panel-2). Across all research context, using no covariates was associated with higher Type II error.  Using all available covariates or selection methods that do not control for *X* performed had higher Type II error rates compared to the other methods when sample sizes were low. This pattern was especially notable for using all covariates and an all covariates linear model without *X*. As sample size increased, the methods performed comparably with respect to Type II error. Using all covariates or an all covariates linear model without *X* for selection produced higher Type II error rates compared to other methods when there was a larger number of covariates available. The all covariates linear model without *X* also produced higher Type II error rates, compared to other methods when the proportion of good covariates was higher.


{{< embed notebooks/mak_figures.qmd#fig-panel-2 >}}




## Parameter Estimates
<!--table rather than distributions.  Three columns for three b.   Doesn't without X show some bias???-->
<!--remind that when assumptions met, parameter estimates are unbiased-->
<!--need to look at this and think a bit more.  Should this also be reported separately for b =0 vs. b <>0 and embedded in those sections above.  Can make the point that we want low type II, .05 type I and unbiased at the start of method or end of intro-->

@tbl-est shows the mean parameter estimate of *X* for all nine methods separately by effect size ($b_x = 0$, $b_x = 0.2$, $b_x = 0.5$). All methods yield unbiased estimates when the effect size is 0. When there is a non-zero effect, p-hacking methods notably inflate the effect size. Other data-driven methods that do not control for X yield slightly biased parameter estimates. Sampling distributions of parameter estimates are available in the supplement.

{{< embed notebooks/mak_tables.qmd#tbl-est >}}

<!--TABLE of TYPE1 and TYPE 2 by method, b and context?  In paper or supplement-->

# Discussion

Our findings reinforce the cautionary warning by Simmons and colleagues [@simmonsFalsepositivePsychologyUndisclosed2011]: p-hacking severely inflates Type I error rate. Simmons et al. simulated an average Type I error rate of 11.7% for a treatment effect when selecting between two potential covariates (*N*=40). In our closest research context (*N*=50, 4 covariates), we observed similar rates of 10.3% (range: 8.4%-13.5%). However, our simulations suggest this estimate may gravely underestimate the magnitude of the problem of p-hacking. It is more realistic that researchers must choose from several candidate covariates. Even when the number of available covariates is modest (i.e., 8-12), our results show that Type I error rates increase to 17.6% (range: 9.4%-32.9%) across contexts. When selecting among 16-20 covariates, Type I error rates, on average, reach a 2.5-fold increase from the 4-covariate context (26.9%, range: 13.9%-46.2%). 

When covariates were completely withheld (i.e., no covariates method), simulated models yielded an average Type I error rate of .05 and unbiased parameter estimates. However, this approach also showed a meaningfully higher Type II error rate compared to the other methods. This is not surprising. Adding covariates correlated with the outcome and uncorrelated with *X* will account for additional variance in the outcome, thus increasing power (or lowering Type II error rate). Perhaps then it is also not surprising that the research contexts with highest discrepancies in Type II error rates between the no covariates method and selection methods include situations with moderate to large sample sizes (*N*=150-400) where degrees of freedom can afford the cost of  additional parameters, and in situations where there are many covariates to choose from with strong correlations to the outcome, where covariates have the most opportunity to reduce unexplained variance in the outcome.

In many instances, using all available covariates selected a priori to data analysis is a reasonable option. Our simulated models showed consistent average Type I error rates of .05 and unbiased parameter estimates. Additionally, on average Type II error rates were only about 2% higher than some of the best performing data-driven selection methods (i.e., all covariates LASSO with *X* and single covariate linear model with *X*). Using all available covariates is easiest because it eliminates the need for selection. However, in certain contexts, such as with small sample sizes or large numbers of covariates this difference is no longer trivial, with some data-driven selection methods yielding Type II error rates as much as 9% lower than the all covariates method. These findings are concerning in that it is not uncommon in psychological research to have 20+ covariates available. Moreover, the high cost of running experimental studies may confine researchers to smaller than ideal sample sizes. Moreover, even with an adequate sample size and modest number of covariates, it may not be wise to walk away from any power to detect an effect. 
<!--JC note: Anchor Type II benefit?  small but how to think about it?   N increase but not linear so specific N?-->

Among the three data-driven selection methods our simulations showed that when the methods did not include the focal variable, *X*, in the model they produced biased parameter estimates. This bias results from the models favoring selecting covariates that are correlated with *X* in the sample despite being uncorrelated in the population. In other words, the models are accounting for variance in the outcome related to *X* by capitalizing on spurious correlations. This bias was largest for the all covariates linear model and trivial with the single covariate linear model.

However, data-driven selection methods that included *X* in the model produced larger Type I error inflation. 
<!--JC notes: 
- Anchor Type I inflation - need simulation
- Why? Flexibility of models? Seen across all methods that include X.-->

Our findings suggest the all covariates linear model is not an optimal data-driven covariate selection method. On average it performed worse than the other data-driven methods. When no true effect existed the all covariates linear model with *X* yielded the highest Type II error compared to the other data-driven selection methods. When there was a true effect, the all covariates linear model with and without *X* showed higher Type II error rates compared to the respective single covariate linear and all covariates LASSO models, with and without *X*. Finally, the all covariates model with *X* produced the most biased parameter estimates, compared to the other data-driven selection methods, when true effects existed. 

The absolute best method for Type II error is all covariates LASSO with *X* included in the model. LASSO is a machine learning algorithm that may be unfamiliar to some psychology researchers. As such, this data-driven selection method may feel overly complicated. We provide code in the supplement<!--need to add this if JC agrees--> to demonstrate how LASSO can be easily implemented in R for covariate selection.

However, the simple single covariate linear model with *X* included in the model may be an equally viable alternative to LASSO. This method performed almost as good as LASSO with regard to Type II error and had lower Type I error compared to LASSO. Moreover, this method is easy to implement as it can be run as a table of correlation or partial correlation values. Both of these methods yielded unbiased parameter estimates and are likely comparable.  Researchers can weigh the trade off between Type I and Type II error and the complexity of these methods in choosing which data-driven selection method to use. And of course, this decision should be pre-registered to avoid introducing additional researcher degrees of freedom into the analyses. 

This study demonstrated the gains in power when using covariates measured before assignment. These findings are likely still applicable to covariates uncorrelated to the focal variable and empirically related to the outcome but measured after assignment. Another use of covariates that we did not address in this study is to statistically control for confounding effects by including additional variables related to both the focal variable and the outcome. This is common practice in situations where *X* cannot be manipulated (e.g., controlling for gender when looking at the effects of parenting style on child school performance). This use of covariates, however, is complicated and covariates must be selected carefully based on theory and causal structure. Unfortunately, controlling for a confounding variable is also not as strong as people tend to believe. Controlling for a confounding variable with a covariate does not fully eliminate its influence but removes the partial effect on the outcome measurable by the model. Given the complex nature of these types of covariates, we considered them to fall outside the scope of the current paper and we plan to address them in a future simulation paper.


# References
