---
title: Evaluating Methods for Covariate Selection
author:
  - name: Lauren Khoury
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: Kendra Wyant
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
abstract: |
  Abstract of paper goes here and can span several lines.
date: last-modified
bibliography: references.bib
bibliographystyle: apa
number-sections: false 
editor_options: 
  chunk_output_type: console
---

# Introduction

## 1

We are often interested in experimental manipulations in Psychology. - Namely, to test if a focal variable has a significant effect on an outcome variable of interest. (give simple example)

## 2

<!--Introduce type 2 error-->

We all know that power is important (frame in terms of type 2 error). - low powered studies don't replicate well (Cohen, replication crisis) - Statistical power is the ability to find a significant effect when one truly does exist. Studies with low power do not replicate well, which led to a replication crisis and an increased awareness of false-positive results. - Type 2 error rate is positively correlated with the "likelihood that a nominally statistically significant finding actually reflects a true effect" - "problems with low power when all other research practices are ideal: low probability of finding true effects, low ppv when an effect is claimed, and exaggerated estimate of the magnitude of the effect when a true effect is discovered (Effect inflation is worst for small, low-powered studies, which can only detect effects that happen to be large.)" (PPV is the probability that a ‘positive’ research finding reflects a true effect (that is, the finding is a true positive)). - statistical power definition - "The statistical power of a test is the probability that it will correctly reject the null hypothesis when the null hypothesis is false (that is, the probability of not committing a type II error or making a false negative decision). The probability of committing a type II error is referred to as the false negative rate (β), and power is equal to 1 – β."

statistical power citations - [@cohenStatisticalPowerAnalysis1988; @cohenPowerPrimer1992]

[@buttonPowerFailureWhy2013] - shows the average statistical power of studies in neuroscience is very low, causing overestimates of effect sizes and low reproducibility of results. They also raise ethical concern that unreliable research is inefficient and wasteful.

## 3

Covariates (measured at baseline before manipulation) to increase power (classic cites?) - keep type 2 error rate down. - Covariates are variables that do not have focal hypotheses, but are added to increase statistical power and model precision of the focal parameter estimate. - They explain some of the variance that might otherwise show up as noise. - Including covariates increases detection of true effects of the manipulated variable of focus. For example, if researchers were testing the effects of a new treatment technique on depression, variables such as age, sex, gender identity, socioeconomic status, and education could be measured at baseline and included in the model as covariates. (continue earlier example with covariates added that intuitively might be related to outcome) This would increase power to find if there is a true significant effect of the new treatment on depression. This increase in power comes from a reduction in the error of the model as covariates account for more unexplained variance in the outcome. This reduces the standard error of the $X$ effect which yields a more precise parameter estimate.

## 4

<!--introduce type 1 error-->

Evidence that p-hacking is problematic (Simons) - Previously, it was common practice to test multiple statistical analyses until statistical significance was reached with only the significant result being reported [@simmonsFalsepositivePsychologyUndisclosed2011]. This repeated testing and selective reporting of significant results came to be known as p-hacking [@urisimonsohnPCurveEffectSize2014]. This led to an increase in "false-positive psychology" where researchers were finding significant effects that did not truly exist. This was motivated by two factors: an inherent publication bias of journals to only publish significant results, leading to researchers striving to reach this result and an ambiguity in decision-making [@simmonsFalsepositivePsychologyUndisclosed2011]. Researchers have various decisions such as sample size of the study, handling of outliers, and including covariates in linear models. There are not always clear guidelines on how to make these decisions.

Ioannidis, J.P. Why most published research findings are false. PLoS Med. 2, e124 (2005). - This study demonstrates that many (and possibly most) of the conclusions drawn from biomedical research are probably false. The reasons for this include using flexible study designs and flexible statistical analyses and running small studies with low statistical power.

Simmons, J.P., Nelson, L.D. & Simonsohn,U. False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychol. Sci. 22, 1359–1366 (2011). - This article empirically illustrates that flexible study designs and data analysis dramatically increase the possibility of obtaining a nominally significant result. However, conclusions drawn from these results are almost certainly false.

## 5

Perhaps shouldn't use covariates. Though as we mentioned this would come with cost to power/increased type 2 error. Another conclusion might be to use all covs (all variables reasonably believed to account for variance in outcome) - reduces degrees of freedom. - The prevalence of possible covariates to measure in the field of Psychology is high, making it difficult for researchers to know how many and which ones to add to a model. - Some measured covariates may be redundant as they explain overlapping variance in the outcome or they may not be related to the outcome at all. - Ideally, only the "good" covariates should be included in statistical models. "Good" covariates are uncorrelated with the focal independent variable and explain variance in the outcome. In experimental work, covariates are measured at baseline before the manipulation and are, therefore, not correlated with the manipulated variable, so the first part is satisfied. The second part is less trivial and needs further exploration.

## 6

We need a method for selecting covariates that doesn't nominally change type 1 error rate. - introduce proposed methods - least absolute shrinkage and selection operator (LASSO)

Our research aims to provide clear and accessible methods for researchers to use to select among a set of covariates by simulating ... across several research settings. 

We replicate that p-hacking is not a statistically valid method and propose eight other methods for selecting covariates. The methods are tested and compared in systematically varied research settings accounting for different effect sizes, sample sizes, amounts of available covariates, proportions of good covariates, and correlations. The methods are compared by their Type I error rates (the probability of rejecting the null hypothesis when no effect exists) and Type II error rates (the probability of failing to reject the null hypothesis when an effect does exist).

# Methods

## Covariate Selection Methods

<!--Think of how we want to refer to the 9 methods, since they are not all covariate selection methods, maybe Regression Methods?-->

We evaluated nine linear regression models. Two models did not use any covariate selection: a linear regression model that used no covariates and a linear regression model that used all available covariates. One model used a statistically invalid method of selecting covariates based on whether they lower the regression p-value (i.e., p-hacking). The remaining six models used three systematic covariate selection methods, selection based on the Pearson correlation coefficient (*r*), a full linear model, and LASSO, controlling for the focal manipulation (i.e., *X*) and without controlling for *X*. A summary of the nine models is presented in @tbl-methods.

{{< embed notebooks/mak_tables.qmd#tbl-methods >}}



## Research Settings
<!--consider adding citations throughout this section to justify some of our decisions-->

We manipulated several variables designed to mimic varying research settings that psychology researchers might be working in. We crossed all levels of each variable to create a total of 540 unique research settings. A summary of these settings is presented in @tbl-dictionary. The variables include:

1. The true population parameter for X. We selected values that represent null ($b_x = 0$), medium ($b_x = 0.3$) and large ($b_x = 0.5$) effect sizes. 

2. The sample size. We chose values for the number of observations that pertain to common sample sizes in experimental research: 50, 100, 150, 200, 300, and 400 observations.

2. The number of covariates available. We selected a wide range of possible scenarios: 4, 8, 12, 16, or 20 covariates.

3. The proportion of "good" covariates. We used varying proportions of good covariates across research settings (.25, .50, and .75) to represent a common reality when a researcher is faced with many covariates, but only some may be related to their outcome.

4. The strength of the relationship between the good covariates and Y. All good covariates were given a moderate and large relationship to *Y*, correlations of 0.3 and 0.5, respectively. Since these good covariates are all correlated with *Y*, it is likeley they are also correlated with each other. Therefore we assigned a moderate 0.3 correlation for all relationships among good covariates. 



{{< embed notebooks/mak_tables.qmd#tbl-dictionary >}}


## Data Analytic Plan

All data analyses were done in R (version 4.4.2). Simulations were run using high-throughput computing resources provided by the University of Wisconsin-Madison Center for High Throughput Computing [@chtc]. 

We ran 40,000 simulations for each research setting. Within each simulation we generated a unique datset that consisted of a dichotomous focal variable (*X*), varying numbers of quantitative covariates, where a subset are correlated with each other and with *Y* (see Research Settings section), and a quantitative outcome (*Y*) calculated by adding the *X* variable multiplied by the effect size (i.e., population parameter) to the *Y* generated from the correlation matrix with the covariates. We fit models from our nine methods on each simulated dataset. From each model we saved out the parameter estimate, standard error, and p-value for *X*. We also calculated true and false positive rates to evaluate the rate at which the model selected covariates correlated with *Y* (i.e., percentage of good covariates selected) and incorrectly selected covariates not correlated with *Y* (i.e., percentage of bad covariates selected).

For research settings where the population parameter for *X* is 0 (i.e., *X* has no effect on *Y*), we report the type I error rate for each method both across and within research settings. For research settings where the population parameter for *X* is 0.3 or 0.5 (i.e., *X* has an effect on *Y*), we report the type II error rate for each method across and within research settings. We also provide sampling distributions of the parameter estimate for *X* across research settings for each method, separately by true effect size (0, 0.3, and 0.5). Detailed tables of range and average error rates for each research setting and true and false positive rates for covariates are available in the supplement.


# Results

## Type I Error

Since this is simulation study, we were able to set the true population parameter for *X* to be zero (i.e., $b_x = 0$). Therefore, any significant result found is a Type I error. @fig-typeI-bar shows the average type I error rate across all research settings for each method. The p-hacking method for selecting covariates, unsurprisingly led to a highly inflated Type I error rate, consistent with the extant research published in the last several years on the connection between researcher degrees of freedom and false positives. Most other methods remained around the expected .05 threshold. There was some elevation in type I errors for methods that controlled for X compared to those, however, it is not clear that these differences are substantial.

{{< embed notebooks/mak_figures.qmd#fig-typeI-bar >}}


We assessed Type 1 error by individual research setting and found several patterns emerged (@fig-typeI-panel). First, small sample sizes are more susceptible to inflated Type I error rates when using covariate selection methods that control for *X*. As sample sizes get larger (i.e., *n* = 300), the methods become comparable. Second, as the number of available covariates to select from increases, type I error rate increases for covariate selection methods that control for *X*, such as a full linear model with *X* and LASSO with *X*, and to a lesser degree partial correlation. Third, there appeared to be no definitive pattern in type I error rate as the proportion of good covariates increased<!--maybe error of LASSO decreases slightly and error of full linear model increases?-->. Similarly we did not see changes in type I error rate as function of the correlation strength between covariates and *Y*.

{{< embed notebooks/mak_figures.qmd#fig-typeI-panel >}}




## Type II Error

In the case of a nonzero $X$ effect, we tested two values for the population parameter for $X$, 0.3 and 0.5 (i.e., $b_x=0.3, b_x=0.5$), so that any non-significant result found is a Type II error. There is no benchmark of expected Type II error as there was for Type I, so the stronger methods will be those with lower Type II error (implying greater statistical power). As the p-hacked method was established as a method that is not statistically valid, it will not be considered in comparisons of Type II error.

The first comparison will look at selection method overall, aggregated across all research settings, for both effect sizes. In Figure [@typeII-bar-03] and Figure [@typeII-bar-05], the proportion of non-significant effects -- the Type II error -- is calculated and displayed. See the Supplemental Material section for the distributions of Type II error of each valid method.

{{< embed notebooks/mak_figures.qmd#fig-typeII-bar-03 >}}

{{< embed notebooks/mak_figures.qmd#fig-typeII-bar-05 >}}

For both effect sizes, similar trends are shown. Fitting a model with no covariates results in the highest Type II error. When further examining the error rates within research settings, the no covariates approach will continue to yield the highest Type II error. There is a large reduction in error when all covariates are included in the model instead of no covariates. Additionally, performing selection of these covariates further reduces Type II error from simply including all covariates.

### By Research Settings

Figure [@fig-typeII-nobs-03] and Figure [@fig-typeII-nobs-05] display the change in Type II error as the number of observations in a sample increases for all eight methods.

{{< embed notebooks/mak_figures.qmd#fig-typeII-nobs-03 >}}

{{< embed notebooks/mak_figures.qmd#fig-typeII-nobs-05 >}}

At the smallest sample size, the methods differ in Type II error, with LASSO and partial correlation approaches yielding the lowest in both effect size cases. As sample size increases, the seven methods (excluding the no covariates method) tend to become indistinguishable.

Figure [@fig-typeII-ncovs-03] and Figure [@fig-typeII-ncovs-05] display the change in Type II error as the number of covariates increases for all eight methods.

{{< embed notebooks/mak_figures.qmd#fig-typeII-ncovs-03 >}}

{{< embed notebooks/mak_figures.qmd#fig-typeII-ncovs-05 >}}

For the smallest number of covariates, the methods are comparable in Type II error, but for larger numbers of covariates, there is a larger gap in performance. As the number of covariates increases, LASSO begins to have the lowest Type II error, followed by the partial correlation and bivariate correlation approaches. Additionally, the method of including all covariates becomes out-performed by the selection methods.

Figure [@fig-typeII-pgoodcovs-03] and Figure [@fig-typeII-pgoodcovs-05] display the change in Type II error as the proportion of good covariates increases for all eight methods.

{{< embed notebooks/mak_figures.qmd#fig-typeII-pgoodcovs-03 >}}

{{< embed notebooks/mak_figures.qmd#fig-typeII-pgoodcovs-05 >}}

For higher proportions of good covariates, LASSO, partial correlation, and bivariate correlation have the lowest Type II error.

Figure [@fig-typeII-rycov-03] and Figure [@fig-typeII-rycov-05] display the change in Type II error as the correlation between $Y$ and the good covariates increases for all eight methods.

{{< embed notebooks/mak_figures.qmd#fig-typeII-rycov-03 >}}

{{< embed notebooks/mak_figures.qmd#fig-typeII-rycov-05 >}}

All methods (excluding the no covariates approach) show similar decreasing trends in Type II error as the correlation between $Y$ and the good covariates increases. The differences between these methods are minimal.



## Parameter Estimates

As mentioned, this section considers a population parameter of *X* set to zero. Figure [@fig-distribution-bx-0] shows the sampling distribution for the parameter estimate of *X* for all nine methods. For further information about the average estimate, see the table in the Supplemental Material section. This table also displays the standard deviation of the estimates from the linear model, the average standard error of the model, and their difference. The standard error of the sampling distribution is expected to match the standard error of the parameter estimates. The table shows a large difference in these values for the p-hacked method and slight differences for LASSO, full linear model, and partial correlation approaches.

{{< embed notebooks/mak_figures.qmd#fig-distribution-bx-0 >}}

This distribution highlights another negative consequence of p-hacking. While its distribution is still centered around zero, it is not normally distributed around zero, emphasizing a biasing of the parameter estimates. The remaining distributions are centered around zero. The model fit with no covariates has the widest distribution, indicating greater variability in its estimate for the $X$ effect.

As mentioned, this section considers two population parameters of $X$ set to 0.3 and 0.5 Figure [@fig-distribution-bx-03] and Figure [@fig-distribution-bx-05] show the sampling distributions for the parameter estimate of $X$ for all eight methods. See the tables in the Supplemental Material section for more information about the average estimates, the standard deviation of the estimates, and the average standard error of the model for both 0.3 and 0.5 effect sizes. The tables show that the bivariate correlation, full linear model without $X$, and LASSO without $X$ approaches slightly underestimate the parameter estimate.

{{< embed notebooks/mak_figures.qmd#fig-distribution-bx-03 >}}

{{< embed notebooks/mak_figures.qmd#fig-distribution-bx-05 >}}

Both distributions show parameter estimates centered around the population values. In both effect size cases, the method including no covariates has a wider distribution, and therefore, more variability in its parameter estimates.

# Discussion

This present study sought to empirically compare methods for covariate selection in linear models. R scripts were used to generate data, fit linear models, and extract and visualize model results. The data generation process was based on full crossings of levels of variables which were chosen to represent common research settings in Psychology. Within each research setting, a unique dataset was simulated 40,000 times using high-throughput computing, with each dataset being used to fit models according to the nine methods discussed.

The first step in evaluating the nine methods, was to establish which methods are statistically valid (i.e., have a Type I error rate of 0.05). We replicated previous findings that p-hacking is not a statistically valid method as it yielded inflated Type I error rates and biased parameter estimates. The remaining eight methods were found to be statistically valid, although LASSO and full linear model show slight inflation.

With this established, the methods could be further compared by their Type II error rates. Lower Type II error can also be considered as higher statistical power. Including all covariates in a model shows a large reduction in Type II error compared to including no covariates in a model. Thus, researchers are encouraged to measure and use covariates. From there, performing a selection of covariates yields further reductions in Type II error compared to simply using all covariates. Overall, LASSO, partial correlation, and bivariate correlation resulted in the lowest Type II errors. However, there are more factors to take into consideration.

Depending on the research setting, researchers may prefer one method over another. As sample size increases (especially past $n=200$), the performance of methods tends to become indistinguishable. For the largest number of covariates tested (20), LASSO had the lowest Type II error, but for smaller numbers, it is comparable to partial correlation. LASSO showed more inflation of Type I error than the partial correlation approach which in turn showed more inflation than the bivariate correlation approach. Among these three, LASSO is the most computationally expensive and will come with a steep learning curve for new users. The bivariate correlation approach showed no inflation of Type I error, but underestimated the parameter estimates for the nonzero $X$ effects. The partial correlation approach did have slight inflation of Type I error, but correctly estimated the parameter estimates for the nonzero effects. The partial correlation approach also had lower Type II error than the bivariate correlation approach, overall and within different research contexts. With this information, researchers should consider what to prioritize depending on their goals.

# References
