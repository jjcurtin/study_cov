@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X},
  doi = {10.1038/nrn3475},
  urldate = {2015-11-26},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  copyright = {{\copyright} 2013 Nature Publishing Group},
  langid = {english},
  file = {C\:\\Users\\kpaquette2\\Zotero\\storage\\5T6BHACZ\\Button et al. - 2013 - Power failure why small sample size undermines th.pdf;C\:\\Users\\kpaquette2\\Zotero\\storage\\AJAGGDXH\\nrn3475.html}
}

@misc{chtc,
  title = {Center for High Throughput Computing},
  author = {{Center for High Throughput Computing}},
  year = {2006},
  publisher = {Center for High Throughput Computing},
  doi = {10.21231/GNT1-HW21}
}

@article{cohenPowerPrimer1992,
  title = {A Power Primer.},
  author = {Cohen, Jacob},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {112},
  number = {1},
  pages = {155--159},
  abstract = {One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product-moment correlation, (3) the difference between independent r s, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation.},
  keywords = {Chi-Square Distribution,psychology,Sample Size},
  file = {C:\Users\kpaquette2\Zotero\storage\522VGJ4A\CohenJ1992a.pdf}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Cohen, Jacob},
  year = {1988},
  month = jul,
  edition = {2 edition},
  publisher = {Routledge},
  address = {Hillsdale, N.J},
  abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
  isbn = {978-0-8058-0283-2},
  langid = {english}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why Most Published Research Findings Are False},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  pmcid = {PMC1182327},
  pmid = {16060722},
  keywords = {Bias (Epidemiology),Cancer risk factors,Data Interpretation Statistical,Finance,Genetic epidemiology,Genetics of disease,Likelihood Functions,Meta-Analysis as Topic,Metaanalysis,Odds Ratio,Publishing,Randomized controlled trials,Reproducibility of Results,Research design,Research Design,Sample Size,Schizophrenia},
  file = {C:\Users\kpaquette2\Zotero\storage\92WMFNX6\Ioannidis - 2005 - Why most published research findings are false.PDF}
}

@article{simmonsFalsepositivePsychologyUndisclosed2011,
  title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  shorttitle = {False-Positive Psychology},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  pmid = {22006061},
  keywords = {Adult,Computer Simulation,Data Collection,Data Interpretation Statistical,Disclosure,Humans,Methodology,Motivated Reasoning,Peer Review Research,Practice Guidelines as Topic,publication,publications,Research Design,Research Personnel,Scientific Method,Statistics as Topic,Type I Errors,Young Adult},
  file = {C\:\\Users\\kpaquette2\\Zotero\\storage\\RA9XMBRC\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf;C\:\\Users\\kpaquette2\\Zotero\\storage\\78GP9PCQ\\1359.html;C\:\\Users\\kpaquette2\\Zotero\\storage\\TFAVSCNQ\\papers.html}
}

@article{urisimonsohnPCurveEffectSize2014,
  title = {P-{{Curve}} and {{Effect Size}}: {{Correcting}} for {{Publication Bias Using Only Significant Results}}},
  author = {{Uri Simonsohn} and {Leif D. Nelson} and {Joseph P. Simmons}},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {666--681},
  doi = {10.1177/1745691614553988}
}
