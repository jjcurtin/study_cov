@article{beachChoosingCovariatesAnalysis1989,
  title = {Choosing Covariates in the Analysis of Clinical Trials},
  author = {Beach, M. L. and Meier, P.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4 Suppl},
  pages = {161S-175S},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90055-x},
  abstract = {Much of the literature on clinical trials emphasizes the importance of adjusting the results for any covariates (baseline variables) for which randomization fails to produce nearly exact balance, but the literature is very nearly devoid of recipes for assessing the consequences of such adjustments. Several years ago, Paul Canner presented an approximate expression for the effect of a covariate adjustment, and he considered its use in the selection of covariates. With the aid of Canner's equation, using both formal analysis and simulation, the impact of covariate adjustment is further explored. Unless tight control over the analysis plans is established in advance, covariate adjustment can lead to seriously misleading inferences. Illustrations from the clinical trials literature are provided.},
  langid = {english},
  pmid = {2605965},
  keywords = {Computer Simulation,Humans,Models Statistical,Monte Carlo Method,Operations Research,Randomized Controlled Trials as Topic,Regression Analysis,Research Design,Selection Bias}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X},
  doi = {10.1038/nrn3475},
  urldate = {2015-11-26},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  copyright = {{\copyright} 2013 Nature Publishing Group},
  langid = {english},
  file = {C\:\\Users\\kpaquette2\\Zotero\\storage\\5T6BHACZ\\Button et al. - 2013 - Power failure why small sample size undermines th.pdf;C\:\\Users\\kpaquette2\\Zotero\\storage\\AJAGGDXH\\nrn3475.html}
}

@misc{chtc,
  title = {Center for High Throughput Computing},
  author = {{Center for High Throughput Computing}},
  year = {2006},
  publisher = {Center for High Throughput Computing},
  doi = {10.21231/GNT1-HW21}
}

@article{cohenPowerPrimer1992,
  title = {A Power Primer.},
  author = {Cohen, Jacob},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {112},
  number = {1},
  pages = {155--159},
  abstract = {One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product-moment correlation, (3) the difference between independent r s, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation.},
  keywords = {Chi-Square Distribution,psychology,Sample Size},
  file = {C:\Users\kpaquette2\Zotero\storage\522VGJ4A\CohenJ1992a.pdf}
}

@article{cohenStatisticalPowerAbnormalsocial1962,
  title = {The Statistical Power of Abnormal-Social Psychological Research: {{A}} Review},
  shorttitle = {The Statistical Power of Abnormal-Social Psychological Research},
  author = {Cohen, Jacob},
  year = {1962},
  journal = {The Journal of Abnormal and Social Psychology},
  volume = {65},
  number = {3},
  pages = {145--153},
  publisher = {American Psychological Association},
  address = {US},
  issn = {0096-851X},
  doi = {10.1037/h0045186},
  file = {C:\Users\kpaquette2\Zotero\storage\S6YKBBX6\doiLanding.html}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Cohen, Jacob},
  year = {1988},
  month = jul,
  edition = {2 edition},
  publisher = {Routledge},
  address = {Hillsdale, N.J},
  abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
  isbn = {978-0-8058-0283-2},
  langid = {english}
}

@article{egbewaleBiasPrecisionStatistical2014,
  title = {Bias, Precision and Statistical Power of Analysis of Covariance in the Analysis of Randomized Trials with Baseline Imbalance: A Simulation Study},
  shorttitle = {Bias, Precision and Statistical Power of Analysis of Covariance in the Analysis of Randomized Trials with Baseline Imbalance},
  author = {Egbewale, Bolaji E. and Lewis, Martyn and Sim, Julius},
  year = {2014},
  month = apr,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {49},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-49},
  urldate = {2025-10-23},
  abstract = {Analysis of variance (ANOVA), change-score analysis (CSA) and analysis of covariance (ANCOVA) respond differently to baseline imbalance in randomized controlled trials. However, no empirical studies appear to have quantified the differential bias and precision of estimates derived from these methods of analysis, and their relative statistical power, in relation to combinations of levels of key trial characteristics. This simulation study therefore examined the relative bias, precision and statistical power of these three analyses using simulated trial data.},
  langid = {english},
  keywords = {Baseline imbalance,Bias,Precision,Randomized controlled trials,Statistical analysis,Statistical power}
}

@article{hernandezCovariateAdjustmentRandomized2004,
  title = {Covariate Adjustment in Randomized Controlled Trials with Dichotomous Outcomes Increases Statistical Power and Reduces Sample Size Requirements},
  author = {Hern{\'a}ndez, Adri{\'a}n V and Steyerberg, Ewout W and Habbema, J. Dik F},
  year = {2004},
  month = may,
  journal = {Journal of Clinical Epidemiology},
  volume = {57},
  number = {5},
  pages = {454--460},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2003.09.014},
  urldate = {2025-10-23},
  abstract = {Objective Randomized controlled trials (RCTs) with dichotomous outcomes may be analyzed with or without adjustment for baseline characteristics (covariates). We studied type I error, power, and potential reduction in sample size with several covariate adjustment strategies. Study Design and Setting Logistic regression analysis was applied to simulated data sets (n=360) with different treatment effects, covariate effects, outcome incidences, and covariate prevalences. Treatment effects were estimated with or without adjustment for a single dichotomous covariate. Strategies included always adjusting for the covariate (``prespecified''), or only when the covariate was predictive or imbalanced. Results We found that the type I error was generally at the nominal level. The power was highest with prespecified adjustment. The potential reduction in sample size was higher with stronger covariate effects (from 3 to 46\%, at 50\% outcome incidence and covariate prevalence) and independent of the treatment effect. At lower outcome incidences and/or covariate prevalences, the reduction was lower. Conclusion We conclude that adjustment for a predictive baseline characteristic may lead to a potentially important increase in power of analyses of treatment effect. Adjusted analysis should, hence, be considered more often for RCTs with dichotomous outcomes.},
  keywords = {Covariate adjustment,Logistic regression,Randomized controlled trials,Sample size,Statistical power,Type I error},
  file = {C:\Users\kpaquette2\Zotero\storage\Z65RTAJV\S0895435603003792.html}
}

@article{hernandezRandomizedControlledTrials2006,
  title = {Randomized {{Controlled Trials With Time-to-Event Outcomes}}: {{How Much Does Prespecified Covariate Adjustment Increase Power}}?},
  shorttitle = {Randomized {{Controlled Trials With Time-to-Event Outcomes}}},
  author = {Hern{\'a}ndez, Adri{\'a}n V. and Eijkemans, Marinus J. C. and Steyerberg, Ewout W.},
  year = {2006},
  month = jan,
  journal = {Annals of Epidemiology},
  volume = {16},
  number = {1},
  pages = {41--48},
  issn = {1047-2797},
  doi = {10.1016/j.annepidem.2005.09.007},
  urldate = {2025-10-23},
  abstract = {Purpose We evaluated the effects of various strategies of covariate adjustment on type I error, power, and potential reduction in sample size in randomized controlled trials (RCTs) with time-to-event outcomes. Methods We used Cox models in simulated data sets with different treatment effects (hazard ratios [HRs] = 1, 1.4, and 1.7), covariate effects (HRs = 1, 2, and 5), covariate prevalences (10\% and 50\%), and censoring levels (no, low, and high). Treatment and a single covariate were dichotomous. We examined the sample size that gives the same power as an unadjusted analysis for three strategies: prespecified, significant predictive, and significant imbalance. Results Type I error generally was at the nominal level. The power to detect a true treatment effect was greater with adjusted than unadjusted analyses, especially with prespecified and significant-predictive strategies. Potential reductions in sample size with a covariate HR between 2 and 5 were between 15\% and 44\% (covariate prevalence 50\%) and between 4\% and 12\% (covariate prevalence 10\%). The significant-imbalance strategy yielded small reductions. The reduction was greater with stronger covariate effects, but was independent of treatment effect, sample size, and censoring level. Conclusions Adjustment for one predictive baseline characteristic yields greater power to detect a true treatment effect than unadjusted analysis, without inflation of type I error and with potentially moderate reductions in sample size. Analysis of RCTs with time-to-event outcomes should adjust for predictive covariates.},
  keywords = {Computer Simulation,Covariate,Power,Proportional Hazards Models,Randomized Controlled Trials,Sample Size,Statistical Data Interpretation},
  file = {C:\Users\kpaquette2\Zotero\storage\2WZNDW3A\S1047279705003248.html}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why Most Published Research Findings Are False},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  pmcid = {PMC1182327},
  pmid = {16060722},
  keywords = {Bias (Epidemiology),Cancer risk factors,Data Interpretation Statistical,Finance,Genetic epidemiology,Genetics of disease,Likelihood Functions,Meta-Analysis as Topic,Metaanalysis,Odds Ratio,Publishing,Randomized controlled trials,Reproducibility of Results,Research design,Research Design,Sample Size,Schizophrenia},
  file = {C:\Users\kpaquette2\Zotero\storage\92WMFNX6\Ioannidis - 2005 - Why most published research findings are false.PDF}
}

@article{kahanRisksRewardsCovariate2014,
  title = {The Risks and Rewards of Covariate Adjustment in Randomized Trials: An Assessment of 12 Outcomes from 8 Studies},
  shorttitle = {The Risks and Rewards of Covariate Adjustment in Randomized Trials},
  author = {Kahan, Brennan C and Jairath, Vipul and Dor{\'e}, Caroline J and Morris, Tim P},
  year = {2014},
  month = apr,
  journal = {Trials},
  volume = {15},
  pages = {139},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-139},
  urldate = {2025-10-23},
  abstract = {Background Adjustment for prognostic covariates can lead to increased power in the analysis of randomized trials. However, adjusted analyses are not often performed in practice. Methods We used simulation to examine the impact of covariate adjustment on 12 outcomes from 8 studies across a range of therapeutic areas. We assessed (1) how large an increase in power can be expected in practice; and (2) the impact of adjustment for covariates that are not prognostic. Results Adjustment for known prognostic covariates led to large increases in power for most outcomes. When power was set to 80\% based on an unadjusted analysis, covariate adjustment led to a median increase in power to 92.6\% across the 12 outcomes (range 80.6 to 99.4\%). Power was increased to over 85\% for 8 of 12 outcomes, and to over 95\% for 5 of 12 outcomes. Conversely, the largest decrease in power from adjustment for covariates that were not prognostic was from 80\% to 78.5\%. Conclusions Adjustment for known prognostic covariates can lead to substantial increases in power, and should be routinely incorporated into the analysis of randomized trials. The potential benefits of adjusting for a small number of possibly prognostic covariates in trials with moderate or large sample sizes far outweigh the risks of doing so, and so should also be considered.},
  pmcid = {PMC4022337},
  pmid = {24755011}
}

@article{pocockSubgroupAnalysisCovariate2002a,
  title = {Subgroup Analysis, Covariate Adjustment and Baseline Comparisons in Clinical Trial Reporting: Current Practiceand Problems},
  shorttitle = {Subgroup Analysis, Covariate Adjustment and Baseline Comparisons in Clinical Trial Reporting},
  author = {Pocock, Stuart J. and Assmann, Susan E. and Enos, Laura E. and Kasten, Linda E.},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {19},
  pages = {2917--2930},
  issn = {1097-0258},
  doi = {10.1002/sim.1296},
  urldate = {2025-10-23},
  abstract = {Clinical trial investigators often record a great deal of baseline data on each patient at randomization. When reporting the trial's findings such baseline data can be used for (i) subgroup analyses which explore whether there is evidence that the treatment difference depends on certain patient characteristics, (ii) covariate-adjusted analyses which aim to refine the analysis of the overall treatment difference by taking account of the fact that some baseline characteristics are related to outcome and may be unbalanced between treatment groups, and (iii) baseline comparisons which compare the baseline characteristics of patients in each treatment group for any possible (unlucky) differences. This paper examines how these issues are currently tackled in the medical journals, based on a recent survey of 50 trial reports in four major journals. The statistical ramifications are explored, major problems are highlighted and recommendations for future practice are proposed. Key issues include: the overuse and overinterpretation of subgroup analyses; the underuse of appropriate statistical tests for interaction; inconsistencies in the use of covariate-adjustment; the lack of clear guidelines on covariate selection; the overuse of baseline comparisons in some studies; the misuses of significance tests for baseline comparability, and the need for trials to have a predefined statistical analysis plan for all these uses of baseline data. Copyright {\copyright} 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {baseline comparisons,clinical trials,covariate adjustment,medical journals,subgroup analysis},
  file = {C:\Users\kpaquette2\Zotero\storage\STXFU5BZ\sim.html}
}

@article{schelchterPosthocSelectionCovariates1985,
  title = {Post-Hoc Selection of Covariates in Randomized Experiments},
  author = {Schelchter, Mark D. and Forsythe, Alan B.},
  year = {1985},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {14},
  number = {3},
  pages = {679--699},
  publisher = {Taylor \& Francis},
  issn = {0361-0926},
  doi = {10.1080/03610928508828942},
  urldate = {2025-10-23},
  abstract = {Monte Carlo methods are used to compere a number of adaptive strategies for deciding which of several covariates to incorporate into the analysis of a randomized experiment.Sixteen selection strategies in three categories are considered: 1)select covariates correlated with the response, 2)select covariates with means differing across groups, and 3)select covariates with means differing across groups that are also correlated with the response. The criteria examined are the type I error rate of the test for equality of adjusted group means and the variance of the estimated treatment effect. These strategies can result in either inflated or deflated type I errors, depending on the method and the population parameters. The adaptive methods in the first category some times yieldpoint estimates of the treatment effect more precise than estimators derive dusing either all or none of the covariates.},
  keywords = {analysis of covariance,Monte Carlo studies,Preliminary tests,stepwise regression,variable selection}
}

@article{simmonsFalsepositivePsychologyUndisclosed2011,
  title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  shorttitle = {False-Positive Psychology},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  pmid = {22006061},
  keywords = {Adult,Computer Simulation,Data Collection,Data Interpretation Statistical,Disclosure,Humans,Methodology,Motivated Reasoning,Peer Review Research,Practice Guidelines as Topic,publication,publications,Research Design,Research Personnel,Scientific Method,Statistics as Topic,Type I Errors,Young Adult},
  file = {C\:\\Users\\kpaquette2\\Zotero\\storage\\RA9XMBRC\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf;C\:\\Users\\kpaquette2\\Zotero\\storage\\78GP9PCQ\\1359.html;C\:\\Users\\kpaquette2\\Zotero\\storage\\TFAVSCNQ\\papers.html}
}

@article{simonsohnPcurveKeyFiledrawer2014,
  title = {P-Curve: {{A}} Key to the File-Drawer.},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  month = apr,
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534--547},
  publisher = {American Psychological Association},
  issn = {1939-2222},
  doi = {10.1037/a0033242},
  urldate = {2025-10-23},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps \&lt; .05). Because only true effects are expected to generate right-skewed p-curves---containing more low (.01s) than high (.04s) significant p values---only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  langid = {english},
  keywords = {Hypothesis Testing,Psychology,Publication Bias,Scientific Communication,Statistics}
}

@article{thompsonCovariateAdjustmentHad2015,
  title = {Covariate Adjustment Had Similar Benefits in Small and Large Randomized Controlled Trials},
  author = {Thompson, Douglas D. and Lingsma, Hester F. and Whiteley, William N. and Murray, Gordon D. and Steyerberg, Ewout W.},
  year = {2015},
  month = sep,
  journal = {Journal of Clinical Epidemiology},
  volume = {68},
  number = {9},
  pages = {1068--1075},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2014.11.001},
  urldate = {2025-10-23},
  abstract = {Objectives Covariate adjustment is a standard statistical approach in the analysis of randomized controlled trials. We aimed to explore whether the benefit of covariate adjustment on statistical significance and power differed between small and large trials, where chance imbalance in prognostic factors necessarily differs. Study Design and Setting We studied two large trial data sets [Global Use of Strategies to Open Occluded Coronary Arteries (GUSTO-I), N~=~30,510 and International Stroke Trial (IST), N~=~18,372] repeatedly drawing random samples (500,000 times) of sizes 300 and 5,000 per arm and simulated each primary outcome using the control arms. We empirically determined the treatment effects required to fix power at 80\% for all unadjusted analyses and calculated the joint probabilities in the discordant cells when cross-classifying adjusted and unadjusted results from logistic regression models (ie, P~{$<~$}0.05 vs. P~{$\geq~$}0.05). Results The power gained from an adjusted analysis for small and large samples was between 5\% and 6\%. Similar proportions of discordance were noted irrespective of the sample size in both the GUSTO-I and the IST data sets. Conclusion The proportions of change in statistical significance from covariate adjustment of strongly prognostic characteristics were the same for small and large trials with similar gains in statistical power. Covariate adjustment is equally recommendable in small and large trials.},
  keywords = {Chance imbalance,Covariate adjustment,Logistic regression analysis,Randomized trial,Sample size,Simulation},
  file = {C:\Users\kpaquette2\Zotero\storage\S7A4SEFL\S089543561400448X.html}
}

@article{urisimonsohnPCurveEffectSize2014,
  title = {P-{{Curve}} and {{Effect Size}}: {{Correcting}} for {{Publication Bias Using Only Significant Results}}},
  author = {{Uri Simonsohn} and {Leif D. Nelson} and {Joseph P. Simmons}},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {666--681},
  doi = {10.1177/1745691614553988}
}

@article{vanbreukelenANCOVAChangeBaseline2006,
  title = {{{ANCOVA}} versus Change from Baseline Had More Power in Randomized Studies and More Bias in Nonrandomized Studies},
  author = {Van Breukelen, Gerard},
  year = {2006},
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  pages = {920--925},
  abstract = {Background and Objective: For inferring a treatment effect from the difference between a treated and untreated group on a quantitative outcome measured before and after treatment, current methods are analysis of covariance (ANCOVA) of the outcome with the baseline as covariate, and analysis of variance (ANOVA) of change from baseline. This article compares both methods on power and bias, for randomized and nonrandomized studies. Methods: The methods are compared by writing both as a regression model and as a repeated measures model, and are applied to a nonrandomized study of preventing depression. Results: In randomized studies both methods are unbiased, but ANCOVA has more power. If treatment assignment is based on the baseline, only ANCOVA is unbiased. In nonrandomized studies with preexisting groups differing at baseline, the two methods cannot both be unbiased, and may contradict each other. In the study of depression, ANCOVA suggests absence, but ANOVA of change suggests presence, of a treatment effect. The methods differ because ANCOVA assumes absence of a baseline difference. Conclusion: In randomized studies and studies with treatment assignment depending on the baseline, ANCOVA must be used. In nonrandomized studies of preexisting groups, ANOVA of change seems less biased than ANCOVA, but two control groups and two baseline measurements are recommended.},
  keywords = {ANCOVA,baseline,change,Change from baseline,different,from,mean,means,measures,Nonrandomized,Nonrandomized studies,regression,Regression to different means,Regression to the mean,repeated,Repeated measures,Studies,the,to},
  file = {C\:\\Users\\kpaquette2\\Zotero\\storage\\6UNUSTP4\\VanBreukelenG2006a.pdf;C\:\\Users\\kpaquette2\\Zotero\\storage\\VDQ5GHKE\\S0895435606000813.html}
}
